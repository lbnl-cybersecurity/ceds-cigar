{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import tan,acos\n",
    "import os\n",
    "import matlab\n",
    "import matlab.engine\n",
    "def start_matlab():\n",
    "    return matlab.engine.start_matlab()\n",
    "def quit_matlab(matlab_engine):\n",
    "    matlab_engine.quit()\n",
    "    \n",
    "def ieee_feeder_mapper(matlab_engine, IeeeFeeder):\n",
    "    FeederMap, Z_in_ohm, Paths, NodeList, LoadList = matlab_engine.ieee_feeder_mapper(IeeeFeeder, nargout=5)\n",
    "    FeederMap = np.array(FeederMap)\n",
    "    Z_in_ohm = np.array(Z_in_ohm)\n",
    "    Paths = np.array(Paths)\n",
    "    NodeList = np.array(NodeList)\n",
    "    LoadList = np.array(LoadList)\n",
    "    return FeederMap, Z_in_ohm, Paths, NodeList[0], LoadList[0]-1\n",
    "\n",
    "def FBSfun(matlab_engine, V0, loads, Z, B):\n",
    "    V, _, S, _ = matlab_engine.FBSfun(float(V0), matlab.double(loads.tolist(),is_complex=True), matlab.double(Z.tolist(), is_complex=True), matlab.double(B.tolist(),is_complex=True), nargout=4)\n",
    "    V = np.array(V, dtype=np.complex).squeeze()\n",
    "    S = np.array(S, dtype=np.complex).squeeze()\n",
    "    return V, S\n",
    "\n",
    "IeeeFeeder = 13\n",
    "matlab_engine = start_matlab()  \n",
    "\n",
    "LoadScalingFactor=2000\n",
    "GenerationScalingFactor=50\n",
    "SlackBusVoltage=1.02\n",
    "power_factor=0.9\n",
    "IncludeSolar=1\n",
    "\n",
    "#Feeder parameters\n",
    "\n",
    "LineNames = ('l_632_633','l_632_645','l_632_671','l_633_634','l_645_646','l_650_632','l_671_680','l_671_684',\n",
    "             'l_671_692','l_684_611','l_684_652','l_692_675','l_u_650')\n",
    "\n",
    "AllBusNames = ('sourcebus',\n",
    "               'load_611','load_634','load_645','load_646','load_652','load_671','load_675','load_692',\n",
    "               'bus_611','bus_634','bus_645','bus_646','bus_652','bus_671','bus_675','bus_692','bus_632',\n",
    "               'bus_633','bus_650','bus_680','bus_684')\n",
    "\n",
    "LoadBusNames = AllBusNames[1:9]\n",
    "BusNames = AllBusNames[9:22]\n",
    "IeeeFeeder = 13\n",
    "\n",
    "LoadList = np.array([6,7,8,13,3,12,11,10])-1\n",
    "NodeList = np.array([650,632,671,680,633,634,645,646,684,611,692,675,652])\n",
    "BusesWithControl = NodeList[LoadList]\n",
    "\n",
    "NumberOfLoads=len(LoadBusNames)\n",
    "NumberOfNodes=len(BusNames)\n",
    "\n",
    "FeederMap, Z_in_ohm, Paths, _, _ = ieee_feeder_mapper(matlab_engine, IeeeFeeder)\n",
    "\n",
    "#Base value calculation\n",
    "Vbase = 4.16e3 #4.16 kV\n",
    "Sbase = 1.0 #500 kVA\n",
    "Zbase = Vbase**2/Sbase\n",
    "Ibase = Sbase/Vbase\n",
    "Z = Z_in_ohm/Zbase\n",
    "\n",
    "#Load Data Pertaining to Loads to create a profile\n",
    "PV_Feeder_model = 10\n",
    "FileDirectoryBase = 'C:\\\\Users\\\\Sy-Toan\\\\ceds-cigar\\\\LBNL_Simulations\\\\testpvnum10\\\\'\n",
    "Time = list(range(1441))\n",
    "TotalTimeSteps = len(Time)\n",
    "QSTS_Data = np.zeros((TotalTimeSteps,4,IeeeFeeder))\n",
    "for node in range(NumberOfLoads):\n",
    "    FileDirectoryExtenstion = 'node_' + str(node+1) + '_pv_' + str(PV_Feeder_model) + '_minute.csv'\n",
    "    FileName = FileDirectoryBase + FileDirectoryExtenstion\n",
    "    MatFile = np.genfromtxt(FileName, delimiter=',')\n",
    "    QSTS_Data[:,:,int(LoadList[node])] = MatFile\n",
    "\n",
    "#Seperate PV Generation Data\n",
    "Generation = QSTS_Data[:,1,:]*GenerationScalingFactor\n",
    "Load = QSTS_Data[:,3,:]*LoadScalingFactor\n",
    "Generation = np.squeeze(Generation)/Sbase\n",
    "Load = np.squeeze(Load)/Sbase\n",
    "MaxGenerationPossible = np.max(Generation, axis=0)\n",
    "\n",
    "#Voltage Observer Parameters and related variable initialization\n",
    "LowPassFilterFrequency = 0.1\n",
    "HighPassFilterFrequency = 1.0\n",
    "Gain_Energy = 1e5\n",
    "TimeStep = 1\n",
    "FilteredOutput_vqvp = np.zeros((TotalTimeSteps,NumberOfNodes))\n",
    "IntermediateOutput_vqvp= np.zeros((TotalTimeSteps,NumberOfNodes))\n",
    "Epsilon_vqvp = np.zeros((TotalTimeSteps,NumberOfNodes))\n",
    "\n",
    "#ZIP load modeling\n",
    "ConstantImpedanceFraction = 0.2\n",
    "ConstantCurrentFraction = 0.05\n",
    "ConstantPowerFraction = 0.75\n",
    "ZIP_demand = np.zeros((TotalTimeSteps,IeeeFeeder,3), dtype=np.complex)\n",
    "\n",
    "for node in range(1,IeeeFeeder):\n",
    "    ZIP_demand[:,node,:] = np.array([ConstantPowerFraction*Load[:,node].astype(complex), \n",
    "                            ConstantCurrentFraction*Load[:,node].astype(complex), \n",
    "                            ConstantImpedanceFraction*Load[:,node].astype(complex)]).T*(1 + 1j*tan(acos(power_factor)))\n",
    "    \n",
    "#Power Flow with QVQP Control Case\n",
    "Sbar =  MaxGenerationPossible * GenerationScalingFactor\n",
    "V_vqvp = np.zeros((IeeeFeeder,TotalTimeSteps), dtype=np.complex)\n",
    "S_vqvp = np.zeros((IeeeFeeder,TotalTimeSteps), dtype=np.complex)\n",
    "IterationCounter_vqvp = np.zeros((IeeeFeeder,TotalTimeSteps))\n",
    "PowerEachTimeStep_vqvp = np.zeros((IeeeFeeder,3), dtype=np.complex)\n",
    "SolarGeneration_vqvp = Generation * GenerationScalingFactor\n",
    "InverterReactivePower = np.zeros(Generation.shape)\n",
    "InverterRealPower = np.zeros(Generation.shape)\n",
    "InverterRateOfChangeLimit = 100 \n",
    "InverterRateOfChangeActivate = 0\n",
    "\n",
    "#Drop Control Parameters\n",
    "VQ_start = 1.01\n",
    "VQ_end = 1.015\n",
    "VP_start = 1.015\n",
    "VP_end = 1.02\n",
    "#VBP is this the config of the control at each time step?\n",
    "VBP = np.full([IeeeFeeder, 4, TotalTimeSteps], np.nan)\n",
    "VBP[:,0,0] = VQ_start\n",
    "VBP[:,1,0] = VQ_end\n",
    "VBP[:,2,0] = VP_start\n",
    "VBP[:,3,0] = VP_end\n",
    "FilteredVoltage = np.zeros(Generation.shape)\n",
    "FilteredVoltageCalc = np.zeros(Generation.shape)\n",
    "InverterLPF = 1\n",
    "ThreshHold_vqvp = 0.25\n",
    "V0 = np.full((TotalTimeSteps, 1), SlackBusVoltage)\n",
    "#Adaptive controller parameters\n",
    "upk = np.zeros(IntermediateOutput_vqvp.shape)\n",
    "uqk = upk\n",
    "kq = 100\n",
    "kp = 100\n",
    "\n",
    "#Delays                 [  1   2   3*  4   5   6*  7*  8*  9* 10* 11* 12* 13*\n",
    "Delay_VoltageSampling = [0, 0,  1, 0, 0,  1,  1,  1,  1,  1,  1,  1,  1]\n",
    "Delay_VBPCurveShift =   [0 ,0 ,2 ,0 ,0 ,2 ,2 ,2 ,2 ,2 ,2 ,2 ,2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voltage_observer(vk, vkm1, psikm1, epsilonkm1, ykm1, f_hp, f_lp, gain, T):\n",
    "    Vmagk = abs(vk)\n",
    "    Vmagkm1 = abs(vkm1)\n",
    "    psik = (Vmagk - Vmagkm1 - (f_hp*T/2-1)*psikm1)/(1+f_hp*T/2)\n",
    "    epsilonk = gain*(psik**2)\n",
    "    yk = (T*f_lp*(epsilonk + epsilonkm1) - (T*f_lp - 2)*ykm1)/(2 + T*f_lp)\n",
    "    return yk, psik, epsilonk\n",
    "\n",
    "def inverter_VoltVarVoltWatt_model(gammakm1,solar_irr,Vk,Vkm1,VBP,T,lpf,Sbar,pkm1,qkm1,ROC_lim,InverterRateOfChangeActivate,ksim,Delay_VoltageSampling):\n",
    "    Vmagk = abs(Vk)\n",
    "    Vmagkm1 = abs(Vkm1) \n",
    "    gammakcalc = (T*lpf*(Vmagk + Vmagkm1) - (T*lpf - 2)*gammakm1)/(2 + T*lpf)\n",
    "    if ksim % Delay_VoltageSampling == 0:\n",
    "        gammakused = gammakcalc\n",
    "    else: \n",
    "        gammakused = gammakm1\n",
    "    \n",
    "    pk = 0\n",
    "    qk = 0\n",
    "    c = 0\n",
    "    q_avail = 0\n",
    "\n",
    "    if solar_irr < 2500:\n",
    "        pk = 0\n",
    "        qk = 0\n",
    "    elif solar_irr >= 2500:\n",
    "        if gammakused <= VBP[2]:\n",
    "            pk = -solar_irr\n",
    "            q_avail = (Sbar**2 - pk**2)**(1/2)\n",
    "            if gammakused <= VBP[0]:\n",
    "                qk = 0\n",
    "            elif gammakused > VBP[0] and gammakused <= VBP[0]:\n",
    "                c = q_avail/(VBP[1] - VBP[0])\n",
    "                qk = c*(gammakused - VBP[0])\n",
    "            else:\n",
    "                qk = q_avail       \n",
    "        elif gammakused > VBP[2] and gammakused < VBP[3]:\n",
    "            d = -solar_irr/(VBP[3] - VBP[2])\n",
    "            pk = -(d*(gammakused - VBP[2]) + solar_irr);\n",
    "            qk = (Sbar**2 - pk**2)**(1/2);      \n",
    "        elif gammakused >= VBP[3]:\n",
    "            qk = Sbar\n",
    "            pk = 0\n",
    "    return qk,pk,gammakused, gammakcalc, c, q_avail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize PowerEachTimeStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the agent!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _state(object):\n",
    "    def __init__(self, PET, V, S, InvReal, InvReact, FV, FVC, Fo, Io, Ep):\n",
    "        self.PET = PET\n",
    "        self.V = V\n",
    "        self.S = S\n",
    "        self.InvReal = InvReal\n",
    "        self.InvReact = InvReact\n",
    "        self.FV = FV\n",
    "        self.FVC = FVC\n",
    "        self.Fo = Fo\n",
    "        self.Io = Io\n",
    "        self.Ep = Ep\n",
    "    \n",
    "    def get_state_agent(self, agent):\n",
    "        arg = (self.PET[agent], np.array([self.V[agent]]), np.array([self.S[agent]]), np.array([self.InvReal[agent]]), np.array([self.InvReact[agent]]),np.array([self.FV[agent]]),np.array([self.FVC[agent]]))\n",
    "        return abs(np.concatenate(arg, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class env(object):\n",
    "    \n",
    "    ######### reset the env ####################\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # a state of env contain: state it is in, terminal or not, in which step it is in\n",
    "        self.stage = 0\n",
    "        PET, V, S = self._init_PET_VS()\n",
    "        InvReal = np.zeros(NumberOfNodes)\n",
    "        InvReact = np.zeros(NumberOfNodes)\n",
    "        FV = np.zeros(NumberOfNodes)\n",
    "        FVC = np.zeros(NumberOfNodes)\n",
    "        Fo = np.zeros(NumberOfNodes)\n",
    "        Io = np.zeros(NumberOfNodes)\n",
    "        Ep = np.zeros(NumberOfNodes)\n",
    "        self.state = _state(PET, V, S, InvReal, InvReact, FV, FVC, Fo, Io, Ep)\n",
    "        self.terminal = False\n",
    "        \n",
    "            \n",
    "        return self.state.get_state_agent(agent)\n",
    "    #############################################\n",
    "    # next state, execute an action #############\n",
    "    #############################################\n",
    "    def step(self, action):\n",
    "        #return next state, reward, terminal or not, precise info\n",
    "        nextPET = self._cal_next_PET()\n",
    "        nextV, nextS = self._cal_next_VS(nextPET)\n",
    "        nextInvReal, nextInvReact, nextFV, nextFVC = self._cal_next_Inv(nextV, action)\n",
    "        nextFo, nextIo, nextEp = self._cal_next_FIE(nextV)\n",
    "        nextState = _state(nextPET, nextV, nextS, nextInvReal, nextInvReact, nextFV, nextFVC, nextFo, nextIo, nextEp) \n",
    "        \n",
    "        #update new state, reward and stage\n",
    "        self.state = nextState\n",
    "        self.reward = -nextFo[agent]\n",
    "        self.stage += 1\n",
    "        \n",
    "        #check if terminal\n",
    "        if (self.stage == TotalTimeSteps-1):\n",
    "            self.terminal = True\n",
    "        \n",
    "        return self.state.get_state_agent(agent), self.reward, self.terminal\n",
    "   ################################################     \n",
    "    def _init_PET_VS(self):\n",
    "        for knode in LoadList:\n",
    "            PowerEachTimeStep_vqvp[knode,:] = np.array([ZIP_demand[0,knode,0] - SolarGeneration_vqvp[0,knode],\n",
    "                                                        ZIP_demand[0,knode,1],\n",
    "                                                        ZIP_demand[0,knode,2]])\n",
    "        V, S = FBSfun(matlab_engine, V0[0,0], PowerEachTimeStep_vqvp, Z, FeederMap)\n",
    "        return PowerEachTimeStep_vqvp, V, S       \n",
    "    \n",
    "    def _cal_next_PET(self):\n",
    "        ksim = self.stage\n",
    "        currentState = self.state\n",
    "        for knode in LoadList:\n",
    "            PowerEachTimeStep_vqvp[knode,:] = np.array([ZIP_demand[ksim+1,knode,0] + currentState.InvReal[knode]\n",
    "                                                         + 1j*currentState.InvReact[knode], \n",
    "                                                        ZIP_demand[ksim+1,knode,1], \n",
    "                                                        ZIP_demand[ksim+1,knode,2]])\n",
    "        return PowerEachTimeStep_vqvp\n",
    "    \n",
    "    def _cal_next_VS(self, nextPET):\n",
    "        ksim = self.stage\n",
    "        V, S = FBSfun(matlab_engine,V0[ksim+1,0], nextPET, Z,FeederMap)\n",
    "        return V, S  \n",
    "    \n",
    "    def _cal_next_Inv(self, nextV, action):\n",
    "        ksim = self.stage\n",
    "        currentState = self.state\n",
    "        \n",
    "        InvReal = np.zeros(NumberOfNodes)\n",
    "        InvReact = np.zeros(NumberOfNodes)\n",
    "        FV = np.zeros(NumberOfNodes)\n",
    "        FVC = np.zeros(NumberOfNodes)\n",
    "        \n",
    "        for knode in LoadList:\n",
    "            InvReact[knode], InvReal[knode], FV[knode], FVC[knode], _, _ = inverter_VoltVarVoltWatt_model(\n",
    "                     currentState.FV[knode], SolarGeneration_vqvp[ksim+1,knode], \n",
    "                     abs(nextV[knode]), abs(currentState.V[knode]), \n",
    "                     action[knode], TimeStep, InverterLPF, \n",
    "                     Sbar[knode], currentState.InvReal[knode], \n",
    "                     currentState.InvReact[knode], InverterRateOfChangeLimit, \n",
    "                     InverterRateOfChangeActivate, ksim+1, Delay_VoltageSampling[knode])\n",
    "        return InvReal, InvReact, FV, FVC\n",
    "    \n",
    "    # ok\n",
    "    def _cal_next_FIE(self, nextV):\n",
    "        currentState = self.state\n",
    "        Fo = np.zeros(NumberOfNodes)\n",
    "        Io = np.zeros(NumberOfNodes)\n",
    "        Ep = np.zeros(NumberOfNodes)\n",
    "        for knode in LoadList:\n",
    "            Fo[knode], Io[knode], Ep[knode] = voltage_observer(nextV[knode], currentState.V[knode], \n",
    "                                                              currentState.Io[knode], currentState.Ep[knode],\n",
    "                                                              currentState.Fo[knode], HighPassFilterFrequency,\n",
    "                                                              LowPassFilterFrequency, Gain_Energy, TimeStep) \n",
    "        return Fo, Io, Ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a buffer for training data\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences \n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing action noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tflearn.fully_connected(net, 300)\n",
    "        t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "        net = tflearn.activation(\n",
    "            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax value\", episode_ave_max_q)\n",
    "    \n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    \n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, args, actor, critic, actor_noise):\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "    \n",
    "    for i in range(int(args['max_episodes'])):\n",
    "        s = env.reset()\n",
    "        \n",
    "        #reset all action\n",
    "        allAction = copy.deepcopy(VBP[:,:,0])\n",
    "        #randomize action of the agent - we dont need to do this\n",
    "        sampl = (np.random.uniform(low=0., high=1., size=(2,)), np.random.uniform(low=1., high=2., size=(2,)))\n",
    "        allAction[agent] = np.concatenate(sampl, axis = 0)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        \n",
    "        for j in range(int(args['max_episode_len'])):\n",
    "            a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "            \n",
    "            translate = np.array([0.5, 0.5, 1, 1])\n",
    "            allAction[agent] = (a + translate).clip(min=0)\n",
    "\n",
    "            s2, r, terminal = env.step(allAction)\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "            \n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = critic.predict_target(\n",
    "                    s2_batch, actor.predict_target(s2_batch))\n",
    "                \n",
    "                y_i = []\n",
    "                for k in range(int(args['minibatch_size'])):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "                \n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "\n",
    "                # Update target networks\n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j)\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} '.format(int(ep_reward), \\\n",
    "                        i, (ep_ave_max_q / float(j))), end=\"\")\n",
    "                print('| Action: ', end=\"\")\n",
    "                print(allAction[agent])\n",
    "                break\n",
    "                \n",
    "#main body\n",
    "\n",
    "def main(args):\n",
    "    with tf.Session() as sess:\n",
    "        simulation = env()\n",
    "        tf.set_random_seed(int(args['random_seed']))\n",
    "\n",
    "        state_dim = 9\n",
    "        action_dim = 4\n",
    "        action_bound = 0.5\n",
    "        # Ensure action bound is symmetric\n",
    "        print(\"creating actor!\")\n",
    "        actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                             float(args['actor_lr']), float(args['tau']),\n",
    "                             int(args['minibatch_size']))\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               float(args['critic_lr']), float(args['tau']),\n",
    "                               float(args['gamma']),\n",
    "                               actor.get_num_trainable_vars())\n",
    "        \n",
    "        actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "        train(sess, simulation, args, actor, critic, actor_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBP = np.full([IeeeFeeder, 4, TotalTimeSteps], np.nan)\n",
    "for i in range(1441):\n",
    "    VBP[:,0,i] = VQ_start\n",
    "    VBP[:,1,i] = VQ_end\n",
    "    VBP[:,2,i] = VP_start\n",
    "    VBP[:,3,i] = VP_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating actor!\n",
      "INFO:tensorflow:Summary name Qmax value is illegal; using Qmax_value instead.\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "| Reward: -591 | Episode: 0 | Qmax: 6.7394 | Action: [0.82414936 0.62333505 0.52408417 1.22318667]\n",
      "| Reward: -805 | Episode: 1 | Qmax: 0.8651 | Action: [0.         0.64364369 0.65809915 1.4857925 ]\n",
      "| Reward: -773 | Episode: 2 | Qmax: 1.7277 | Action: [0.4204722  1.70825403 0.70877155 2.58362306]\n",
      "| Reward: -690 | Episode: 3 | Qmax: 2.5928 | Action: [0.         0.84772043 0.32224816 2.24536558]\n",
      "| Reward: -541 | Episode: 4 | Qmax: 1.7737 | Action: [0.         0.58182161 0.         1.87200727]\n",
      "| Reward: -376 | Episode: 5 | Qmax: 1.4622 | Action: [0.33198809 1.6754384  0.09113275 2.20567474]\n",
      "| Reward: -671 | Episode: 6 | Qmax: 3.2254 | Action: [0.         1.12618496 0.36401602 3.65232995]\n",
      "| Reward: -587 | Episode: 7 | Qmax: 3.1704 | Action: [0.24759996 0.97696283 0.4051604  0.53290411]\n",
      "| Reward: -240 | Episode: 8 | Qmax: 3.1115 | Action: [0.         0.70438737 1.81404561 0.88277321]\n",
      "| Reward: -632 | Episode: 9 | Qmax: 3.0207 | Action: [0.49579653 0.92576377 0.8566342  1.43834013]\n",
      "| Reward: -717 | Episode: 10 | Qmax: 3.0597 | Action: [0.         1.18618103 0.4046016  1.86183091]\n",
      "| Reward: -571 | Episode: 11 | Qmax: 0.0122 | Action: [0.         0.93412382 0.99057472 1.6266967 ]\n",
      "| Reward: -707 | Episode: 12 | Qmax: -0.3400 | Action: [0.00924778 0.57206236 0.4964229  1.80020192]\n",
      "| Reward: -551 | Episode: 13 | Qmax: -0.4394 | Action: [0.         1.05230986 0.73097315 1.64707229]\n",
      "| Reward: -467 | Episode: 14 | Qmax: -0.5188 | Action: [0.90636503 1.36997247 0.         1.68961186]\n",
      "| Reward: -410 | Episode: 15 | Qmax: -0.5958 | Action: [0.52792262 1.34535842 1.10168656 1.51558293]\n",
      "| Reward: -440 | Episode: 16 | Qmax: -0.6730 | Action: [0.         0.50990954 0.         1.01434859]\n",
      "| Reward: -428 | Episode: 17 | Qmax: -0.7424 | Action: [0.         1.3967227  0.50786231 1.17684596]\n",
      "| Reward: -567 | Episode: 18 | Qmax: -0.8107 | Action: [0.         1.85534564 0.79415018 1.16042664]\n",
      "| Reward: -797 | Episode: 19 | Qmax: -0.8791 | Action: [0.         0.98033466 0.02213524 1.29373838]\n",
      "| Reward: -485 | Episode: 20 | Qmax: -0.9464 | Action: [0.         1.06684502 0.02981946 2.24905184]\n",
      "| Reward: -621 | Episode: 21 | Qmax: -1.0131 | Action: [0.         0.81684183 0.76625176 1.31493605]\n",
      "| Reward: -610 | Episode: 22 | Qmax: -1.0790 | Action: [0.01652236 1.3972551  0.15227123 0.81433511]\n",
      "| Reward: -233 | Episode: 23 | Qmax: -1.1489 | Action: [0.23149335 1.97903453 0.04188222 0.07952526]\n",
      "| Reward: -244 | Episode: 24 | Qmax: -1.2194 | Action: [0.         0.92757262 0.         0.6618622 ]\n",
      "| Reward: -338 | Episode: 25 | Qmax: -1.2953 | Action: [0.         0.7342475  0.7601168  1.29086866]\n",
      "| Reward: -619 | Episode: 26 | Qmax: -1.3787 | Action: [0.         1.35709036 0.71974686 2.2480158 ]\n",
      "| Reward: -825 | Episode: 27 | Qmax: -1.4493 | Action: [0.         0.55193845 0.30181153 1.34709409]\n",
      "| Reward: -339 | Episode: 28 | Qmax: -1.4880 | Action: [0.         1.52223733 0.         1.52112233]\n",
      "| Reward: -480 | Episode: 29 | Qmax: -1.5323 | Action: [0.01418132 0.2071788  0.38471686 1.80040585]\n",
      "| Reward: -527 | Episode: 30 | Qmax: -1.5735 | Action: [0.12083458 1.08219249 2.08446567 2.41619014]\n",
      "| Reward: -688 | Episode: 31 | Qmax: -1.6171 | Action: [0.         1.23707303 0.53192009 1.35980183]\n",
      "| Reward: -867 | Episode: 32 | Qmax: -1.6603 | Action: [0.17099442 0.88977159 1.27141912 1.47286522]\n",
      "| Reward: -638 | Episode: 33 | Qmax: -1.7084 | Action: [0.         0.91049052 1.71434586 0.91751046]\n",
      "| Reward: -554 | Episode: 34 | Qmax: -1.7536 | Action: [0.         0.95599996 0.24089973 1.3886065 ]\n",
      "| Reward: -339 | Episode: 35 | Qmax: -1.8001 | Action: [0.         1.41016186 0.49165796 1.09572868]\n",
      "| Reward: -720 | Episode: 36 | Qmax: -1.8417 | Action: [0.         0.85883011 1.09946188 1.42456487]\n",
      "| Reward: -481 | Episode: 37 | Qmax: -1.8822 | Action: [0.50190795 0.         0.26715236 1.44495135]\n",
      "| Reward: -841 | Episode: 38 | Qmax: -1.9269 | Action: [0.35180615 1.91420227 0.         2.57078065]\n",
      "| Reward: -513 | Episode: 39 | Qmax: -1.9716 | Action: [0.         0.05334445 0.         0.86760854]\n",
      "| Reward: -333 | Episode: 40 | Qmax: -2.0194 | Action: [0.         1.47217412 0.67678019 1.16753998]\n",
      "| Reward: -324 | Episode: 41 | Qmax: -2.0664 | Action: [0.34379911 1.93822266 0.2409565  1.56821482]\n",
      "| Reward: -774 | Episode: 42 | Qmax: -2.1147 | Action: [0.25580499 1.78463856 0.17046664 2.46237557]\n",
      "| Reward: -697 | Episode: 43 | Qmax: -2.1566 | Action: [0.35329877 0.89005345 0.23009505 2.13814468]\n",
      "| Reward: -664 | Episode: 44 | Qmax: -2.1964 | Action: [0.11959724 1.58425919 0.         2.13363682]\n",
      "| Reward: -697 | Episode: 45 | Qmax: -2.2395 | Action: [0.         0.86764593 0.         2.27982604]\n",
      "| Reward: -706 | Episode: 46 | Qmax: -2.2904 | Action: [0.15825708 0.02004411 0.66402516 1.15815613]\n",
      "| Reward: -235 | Episode: 47 | Qmax: -2.3340 | Action: [0.00462806 1.10520041 1.49867181 1.77986802]\n",
      "| Reward: -741 | Episode: 48 | Qmax: -2.3772 | Action: [0.         0.54041555 0.         1.64723472]\n",
      "| Reward: -487 | Episode: 49 | Qmax: -2.4222 | Action: [0.         1.18108716 0.         0.93858346]\n",
      "| Reward: -584 | Episode: 50 | Qmax: -2.4639 | Action: [0.73106999 0.92635031 0.26633662 1.23886227]\n",
      "| Reward: -551 | Episode: 51 | Qmax: -2.5051 | Action: [0.75736215 1.20610825 0.612322   0.46922925]\n",
      "| Reward: -233 | Episode: 52 | Qmax: -2.5564 | Action: [0.         1.70737127 0.57120893 0.90489854]\n",
      "| Reward: -580 | Episode: 53 | Qmax: -2.6022 | Action: [0.         1.29057907 0.07129258 2.25529449]\n",
      "| Reward: -919 | Episode: 54 | Qmax: -2.6476 | Action: [0.41528582 1.70601822 0.15013191 1.38064264]\n",
      "| Reward: -498 | Episode: 55 | Qmax: -2.6951 | Action: [0.         1.02227877 0.         1.58122504]\n",
      "| Reward: -653 | Episode: 56 | Qmax: -2.7406 | Action: [0.17968271 1.07468068 0.71965837 2.38125307]\n",
      "| Reward: -431 | Episode: 57 | Qmax: -2.7929 | Action: [0.         1.31634249 0.93998613 1.8781495 ]\n",
      "| Reward: -668 | Episode: 58 | Qmax: -2.8354 | Action: [0.         1.88303453 0.67639411 0.80780621]\n",
      "| Reward: -335 | Episode: 59 | Qmax: -2.8721 | Action: [0.11005409 0.36439469 1.16882679 1.40555275]\n",
      "| Reward: -794 | Episode: 60 | Qmax: -2.9146 | Action: [0.02752878 0.57638268 0.81860401 1.9345905 ]\n",
      "| Reward: -804 | Episode: 61 | Qmax: -2.9515 | Action: [1.12619135 1.29477273 0.         2.16099443]\n",
      "| Reward: -613 | Episode: 62 | Qmax: -2.9961 | Action: [0.         0.6990731  0.91467681 1.351956  ]\n",
      "| Reward: -278 | Episode: 63 | Qmax: -3.0352 | Action: [1.02412304 0.99309488 0.70194463 1.45966725]\n",
      "| Reward: -686 | Episode: 64 | Qmax: -3.0709 | Action: [0.07635899 1.17521088 0.         1.53694494]\n",
      "| Reward: -531 | Episode: 65 | Qmax: -3.1142 | Action: [0.04600381 0.3726223  0.         2.60760219]\n",
      "| Reward: -655 | Episode: 66 | Qmax: -3.1471 | Action: [1.24251665 0.15724956 0.85620449 1.82698568]\n",
      "| Reward: -483 | Episode: 67 | Qmax: -3.1808 | Action: [0.32127268 0.60323205 0.8710889  1.05525471]\n",
      "| Reward: -658 | Episode: 68 | Qmax: -3.2110 | Action: [0.27800543 0.28071791 0.01407258 0.90478465]\n",
      "| Reward: -270 | Episode: 69 | Qmax: -3.2481 | Action: [0.27706504 0.85873606 0.         1.18113811]\n",
      "| Reward: -414 | Episode: 70 | Qmax: -3.2880 | Action: [0.26856424 1.56471271 0.21965465 0.98878667]\n",
      "| Reward: -639 | Episode: 71 | Qmax: -3.3204 | Action: [0.         1.23578695 0.69037582 0.99953834]\n",
      "| Reward: -848 | Episode: 72 | Qmax: -3.3545 | Action: [0.16582043 0.22098076 1.26178979 1.55340929]\n",
      "| Reward: -703 | Episode: 73 | Qmax: -3.3911 | Action: [0.         0.         0.1544797  0.09952942]\n",
      "| Reward: -399 | Episode: 74 | Qmax: -3.4328 | Action: [0.         0.59092388 0.         2.13593571]\n",
      "| Reward: -310 | Episode: 75 | Qmax: -3.4687 | Action: [0.         0.98328023 0.         0.81939177]\n",
      "| Reward: -236 | Episode: 76 | Qmax: -3.5079 | Action: [0.         1.04710055 0.40924334 1.06966085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Reward: -505 | Episode: 77 | Qmax: -3.5440 | Action: [0.         1.373091   0.9178256  1.00612945]\n",
      "| Reward: -304 | Episode: 78 | Qmax: -3.5794 | Action: [0.         0.94638633 0.93858992 0.76726811]\n",
      "| Reward: -860 | Episode: 79 | Qmax: -3.6187 | Action: [0.26093857 0.18730786 1.16392102 1.07008486]\n",
      "| Reward: -789 | Episode: 80 | Qmax: -3.6545 | Action: [0.         1.3968265  1.54461614 0.73677739]\n",
      "| Reward: -601 | Episode: 81 | Qmax: -3.7005 | Action: [0.         1.42234518 0.79459189 2.02451942]\n",
      "| Reward: -785 | Episode: 82 | Qmax: -3.7394 | Action: [0.         0.77690011 0.         1.50423196]\n",
      "| Reward: -389 | Episode: 83 | Qmax: -3.7778 | Action: [0.43551974 0.9358445  0.         2.24021299]\n",
      "| Reward: -460 | Episode: 84 | Qmax: -3.8185 | Action: [0.         0.15676048 0.71400082 1.66099122]\n",
      "| Reward: -756 | Episode: 85 | Qmax: -3.8527 | Action: [0.         1.0702313  0.72849911 0.97237797]\n",
      "| Reward: -464 | Episode: 86 | Qmax: -3.8895 | Action: [0.46135798 1.48509715 0.11366565 1.74785754]\n",
      "| Reward: -623 | Episode: 87 | Qmax: -3.9366 | Action: [0.         1.77113944 1.28831007 1.24800566]\n",
      "| Reward: -683 | Episode: 88 | Qmax: -3.9797 | Action: [0.         0.87068414 0.45882073 1.66753922]\n",
      "| Reward: -419 | Episode: 89 | Qmax: -4.0198 | Action: [0.         0.14874078 1.35572975 1.27504526]\n",
      "| Reward: -814 | Episode: 90 | Qmax: -4.0664 | Action: [0.         0.68059303 0.3077851  1.52161879]\n",
      "| Reward: -616 | Episode: 91 | Qmax: -4.1159 | Action: [0.         2.27797102 0.74727347 1.98632834]\n",
      "| Reward: -743 | Episode: 92 | Qmax: -4.1581 | Action: [0.         1.46244183 0.         2.27798091]\n",
      "| Reward: -459 | Episode: 93 | Qmax: -4.1969 | Action: [0.08471662 1.69948668 0.45179509 1.86352988]\n",
      "| Reward: -457 | Episode: 94 | Qmax: -4.2446 | Action: [0.         1.76342134 0.46811577 1.54485122]\n",
      "| Reward: -245 | Episode: 95 | Qmax: -4.2811 | Action: [0.1123124  1.14482505 0.2879631  1.83957984]\n",
      "| Reward: -623 | Episode: 96 | Qmax: -4.3316 | Action: [0.38640002 0.14475097 0.         1.02869876]\n",
      "| Reward: -369 | Episode: 97 | Qmax: -4.3713 | Action: [0.05592614 1.24059826 0.67499047 1.86734117]\n",
      "| Reward: -649 | Episode: 98 | Qmax: -4.4165 | Action: [0.         1.25082922 0.97896612 0.89479571]\n",
      "| Reward: -595 | Episode: 99 | Qmax: -4.4517 | Action: [0.70531832 1.39622143 0.45878906 0.92919142]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-03288dc81f50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#init action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mallAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVBP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#need to run simulation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-a96ada921615>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mactor_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrnsteinUhlenbeckActionNoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-a96ada921615>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sess, env, args, actor, critic, actor_noise)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# Update the critic given the targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 predicted_q_value, _ = critic.train(\n\u001b[1;32m---> 43\u001b[1;33m                     s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mep_ave_max_q\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_q_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-e21dd9fd8959>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, inputs, action, predicted_q_value)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_q_value\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredicted_q_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         })\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import copy\n",
    "args = {}\n",
    "args['actor_lr'] = 0.0001\n",
    "args['critic_lr'] = 0.001\n",
    "args['gamma'] = 0.99\n",
    "args['tau'] = 0.001\n",
    "args['buffer_size'] = 50000\n",
    "args['minibatch_size'] = 200\n",
    "args['random_seed'] = 1234\n",
    "args['max_episodes'] = 5000\n",
    "args['max_episode_len'] = TotalTimeSteps\n",
    "args['summary_dir'] = os.getcwd()\n",
    "args['random_seed'] = 1234\n",
    "#in this chapter, we only want to control 1 agent, other agents have a constant VBP\n",
    "#agent \n",
    "agent = 2\n",
    "#init action \n",
    "allAction = copy.deepcopy(VBP[:,:,0]) #need to run simulation\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69167284, 0.96219075, 1.09764676, 1.84317506])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampl = (np.random.uniform(low=0., high=1., size=(2,)), np.random.uniform(low=1., high=2., size=(2,)))\n",
    "np.concatenate(sampl, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TotalTimeSteps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-be6386970620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mksim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTotalTimeSteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m###############calculate new simulation#####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#CALCULATE NET ZIP LOADS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TotalTimeSteps' is not defined"
     ]
    }
   ],
   "source": [
    "for ksim in range(TotalTimeSteps):\n",
    "    ############################################################\n",
    "    ###############calculate new simulation#####################\n",
    "    ############################################################\n",
    "    #CALCULATE NET ZIP LOADS\n",
    "    for node_iter in range(NumberOfLoads):\n",
    "        knode = LoadList[node_iter]\n",
    "        if ksim == 0:\n",
    "            PowerEachTimeStep_vqvp[knode,:] = np.array([ZIP_demand[ksim,knode,0] - SolarGeneration_vqvp[ksim,knode],\n",
    "                                                        ZIP_demand[ksim,knode,1],\n",
    "                                                        ZIP_demand[ksim,knode,2]])\n",
    "        else:\n",
    "            PowerEachTimeStep_vqvp[knode,:] = np.array([ZIP_demand[ksim,knode,0] + InverterRealPower[ksim-1,knode]\n",
    "                                                         + 1j*InverterReactivePower[ksim-1,knode], \n",
    "                                                        ZIP_demand[ksim,knode,1], \n",
    "                                                        ZIP_demand[ksim,knode,2]])\n",
    "    #RUN FORWARD-BACKWARD SWEEP\n",
    "    V_vqvp[:,ksim],S_vqvp[:,ksim] = FBSfun(matlab_engine,V0[ksim,0],PowerEachTimeStep_vqvp,Z,FeederMap)\n",
    "    \n",
    "    #RUN INVERTER FUNCTION TO OUTPUT P/Q\n",
    "    if (0 < ksim < TotalTimeSteps-1):\n",
    "        for node_iter in range(NumberOfLoads):\n",
    "            knode = LoadList[node_iter]\n",
    "            InverterReactivePower[ksim,knode],InverterRealPower[ksim,knode],FilteredVoltage[ksim,knode],FilteredVoltageCalc[ksim,knode],_, _ = inverter_VoltVarVoltWatt_model(\n",
    "                FilteredVoltage[ksim-1,knode], SolarGeneration_vqvp[ksim,knode], \n",
    "                abs(V_vqvp[knode,ksim]), abs(V_vqvp[knode,ksim-1]), \n",
    "                VBP[knode,:,ksim], TimeStep, InverterLPF, \n",
    "                Sbar[knode], InverterRealPower[ksim-1,knode], \n",
    "                InverterReactivePower[ksim-1,knode], InverterRateOfChangeLimit, \n",
    "                InverterRateOfChangeActivate, ksim, Delay_VoltageSampling[knode])\n",
    "    \n",
    "    #RUN OBSERVER FUNCTION\n",
    "    for node_iter in range(NumberOfLoads):\n",
    "        knode = LoadList[node_iter]\n",
    "        if (ksim > 0):\n",
    "            FilteredOutput_vqvp[ksim,knode],IntermediateOutput_vqvp[ksim,knode],Epsilon_vqvp[ksim,knode] = voltage_observer(V_vqvp[knode,ksim], V_vqvp[knode,ksim-1], \n",
    "                                                        IntermediateOutput_vqvp[ksim-1,knode], Epsilon_vqvp[ksim-1,knode], \n",
    "                                                        FilteredOutput_vqvp[ksim-1,knode], HighPassFilterFrequency, \n",
    "                                                        LowPassFilterFrequency, Gain_Energy, TimeStep)        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
