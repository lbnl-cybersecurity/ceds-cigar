{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Converged. Proceeding to Next Step.\n",
      "OpenDSS Model Compliation Done.\n",
      "Reading Data for Pecan Street is done.\n",
      "Starting Interpolation...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from DSSStartup import DSSStartup\n",
    "from setInfo import *\n",
    "from getInfo import *\n",
    "import matplotlib.pyplot as plt\n",
    "from math import tan,acos\n",
    "import copy\n",
    "import pandas as pd\n",
    "import time\n",
    "from replay_buffer import ReplayBuffer\n",
    "import numpy as np\n",
    "#######################################################\n",
    "#######################################################\n",
    "###Global variable initialization and error checking###\n",
    "#######################################################\n",
    "#######################################################\n",
    "Sbase=1\n",
    "LoadScalingFactor = 1.5\n",
    "GenerationScalingFactor = 5 \n",
    "SlackBusVoltage = 1.04 \n",
    "NoiseMultiplyer= 0\n",
    "\n",
    "#Set simulation analysis period - the simulation is from StartTime to EndTime\n",
    "StartTime = 42900\n",
    "EndTime = 44000\n",
    "EndTime += 1 # creating a list, last element does not count, so we increase EndTime by 1\n",
    "#Set hack parameters\n",
    "TimeStepOfHack = 300\n",
    "PercentHacked = np.array([0,0,0,0,0, 0,0,.5,0,0,.5,.5,.5,.5,.5,0,0,.5, 0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "#PercentHacked = np.array([0,0,0,0,0, .1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1,.1, 0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "#Set initial VBP parameters for uncompromised inverters\n",
    "VQ_start = 1.01\n",
    "VQ_end = 1.03\n",
    "VP_start = 1.03\n",
    "VP_end = 1.05\n",
    "\n",
    "#Set delays for each node\n",
    "\n",
    "Delay_VoltageSampling = np.array([0,0,0,0,0, 10,10,10,10,10,10,10,10,10,10,10,10,10, 0,0,0,0,0,0,0,0,0,0,0,0,0]) \n",
    "Delay_VBPCurveShift =   np.array([0,0,0,0,0, 60,60,60,60,60,60,60,60,60,60,60,60,60, 0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "#Set observer voltage threshold\n",
    "ThreshHold_vqvp = 0.25\n",
    "power_factor=0.9\n",
    "pf_converted=tan(acos(power_factor))\n",
    "Number_of_Inverters = 13 #even feeder is 34Bus, we only have 13 inverters\n",
    "\n",
    "\n",
    "#Error checking of the global variable -- TODO: add error handling here!\n",
    "if EndTime < StartTime or EndTime < 0 or StartTime < 0:\n",
    "    print('Setup Simulation Times Appropriately.')\n",
    "if NoiseMultiplyer < 0:\n",
    "    print('Setup Noise Multiplyer Correctly.')\n",
    "    \n",
    "DSSStart = DSSStartup()\n",
    "DSSText =DSSStart['dsstext']\n",
    "DSSSolution = DSSStart['dsssolution']\n",
    "DSSCircuit = DSSStart['dsscircuit']\n",
    "DSSObj = DSSStart['dssobj']\n",
    "DSSMon = DSSCircuit.Monitors\n",
    "DSSText.command = 'Compile C:\\\\feeders\\\\feeder34_B_NR\\\\feeder34_B_NR.dss'\n",
    "DSSSolution.Solve()\n",
    "if not DSSSolution.Converged:\n",
    "    print('Initial Solution Not Converged. Check Model for Convergence')\n",
    "else:\n",
    "    print('Initial Model Converged. Proceeding to Next Step.')\n",
    "    #Doing this solve command is required for GridPV, that is why the monitors\n",
    "    #go under a reset process\n",
    "    DSSMon.ResetAll\n",
    "    setSolutionParams(DSSObj,'daily',1,1,'off',1000000,30000)\n",
    "    #Easy process to get all names and count of loads, a trick to avoid\n",
    "    #some more lines of code\n",
    "    TotalLoads=DSSCircuit.Loads.Count\n",
    "    AllLoadNames=DSSCircuit.Loads.AllNames\n",
    "    print('OpenDSS Model Compliation Done.')\n",
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "####         Load data from file                    ###\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "#Retrieving the data from the load profile\n",
    "TimeResolutionOfData=10 #resolution in minute\n",
    "#Get the data from the Testpvnum folder\n",
    "#Provide Your Directory - move testpvnum10 from github to drive C: \n",
    "FileDirectoryBase ='C:\\\\feeders\\\\testpvnum10\\\\';\n",
    "QSTS_Time = list(range(1441)) #This can be changed based on the available data - for example, 1440 timesteps\n",
    "QSTS_Data = np.zeros((len(QSTS_Time) ,4,TotalLoads)) #4 columns as there are four columns of data available in the .mat file\n",
    "\n",
    "for node in range(TotalLoads):\n",
    "    #This is created manually according to the naming of the folder\n",
    "    FileDirectoryExtension = 'node_' + str(node+1) + '_pv_' +str(TimeResolutionOfData) + '_minute.csv'\n",
    "    #The total file directory\n",
    "    FileName = FileDirectoryBase + FileDirectoryExtension\n",
    "    #Load the file\n",
    "    MatFile = np.genfromtxt(FileName, delimiter=',')    \n",
    "    QSTS_Data[:,:,node] = MatFile #Putting the loads to appropriate nodes according to the loadlist\n",
    "    \n",
    "Generation = QSTS_Data[:,1,:]*GenerationScalingFactor #solar generation\n",
    "Load = QSTS_Data[:,3,:]*LoadScalingFactor #load demand\n",
    "Generation = np.squeeze(Generation)/Sbase  #To convert to per unit, it should not be multiplied by 100\n",
    "Load = np.squeeze(Load)/Sbase\n",
    "print('Reading Data for Pecan Street is done.')\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "#### Interpolate to change data from minutes to seconds ####\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "print('Starting Interpolation...')\n",
    "\n",
    "#interpolation for the whole period...\n",
    "Time = list(range(StartTime,EndTime))\n",
    "TotalTimeSteps = len(Time)\n",
    "LoadSeconds = np.empty([3600*24, TotalLoads])\n",
    "GenerationSeconds = np.empty([3600*24, TotalLoads])\n",
    "# Interpolate to get minutes to seconds\n",
    "for node in range(TotalLoads): # i is node\n",
    "    t_seconds = np.linspace(1,len(Load[:,node]), int(3600*24/1))\n",
    "    f = interp1d(range(len(Load[:,node])), Load[:,node], kind='cubic', fill_value=\"extrapolate\")\n",
    "    LoadSeconds[:,node] = f(t_seconds) #spline method in matlab equal to Cubic Spline -> cubic\n",
    "    \n",
    "    f = interp1d(range(len(Generation[:,node])), Generation[:,node], kind='cubic', fill_value=\"extrapolate\")\n",
    "    GenerationSeconds[:,node]= f(t_seconds)\n",
    "\n",
    "# Initialization\n",
    "# then we take out only the window we want...\n",
    "LoadSeconds = LoadSeconds[StartTime:EndTime,:]\n",
    "GenerationSeconds = GenerationSeconds[StartTime:EndTime,:]\n",
    "Load = LoadSeconds\n",
    "Generation = GenerationSeconds\n",
    "Origin_Load = LoadSeconds\n",
    "Origin_Generation = GenerationSeconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "#### Function for simulation################################\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "def inverter_qp_injection(counter, Vk, Vkm1, gammakm1, solar_irr, VBP, Sbar, dv, T=1, lpf=1):\n",
    "    pk = 0\n",
    "    qk = 0\n",
    "    c = 0\n",
    "    q_avail = 0\n",
    "\n",
    "    solar_range = 5\n",
    "\n",
    "    Vmagk = abs(Vk)\n",
    "    Vmagkm1 = abs(Vkm1)\n",
    "    gammakcalc = (T*lpf*(Vmagk + Vmagkm1) - (T*lpf - 2)*gammakm1)/(2 + T*lpf)\n",
    "\n",
    "    if counter % dv == 0:\n",
    "        gammakused = gammakcalc\n",
    "    else :\n",
    "        gammakused = gammakm1\n",
    "\n",
    "    if solar_irr < solar_range:\n",
    "        pk = 0\n",
    "        qk = 0\n",
    "    elif solar_irr >= solar_range:\n",
    "        if gammakused <= VBP[2]:\n",
    "            pk = -solar_irr\n",
    "            q_avail = (Sbar**2 - pk**2)**(1/2)\n",
    "            if gammakused <= VBP[0]:\n",
    "                qk = 0\n",
    "            elif gammakused > VBP[0] and gammakused <= VBP[1]:\n",
    "                c = q_avail/(VBP[1] - VBP[0])\n",
    "                qk = c*(gammakused - VBP[0])\n",
    "            else:\n",
    "                qk = q_avail\n",
    "        elif gammakused > VBP[2] and gammakused < VBP[3]:\n",
    "            d = -solar_irr/(VBP[3] - VBP[2])\n",
    "            pk = -(d*(gammakused - VBP[2]) + solar_irr)\n",
    "            qk = (Sbar**2 - pk**2)**(1/2)\n",
    "        elif gammakused >= VBP[3]:\n",
    "            qk = Sbar\n",
    "            pk = 0\n",
    "    return qk,pk, gammakused\n",
    "    \n",
    "def voltage_observer(vk, vkm1, psikm1, epsilonkm1, ykm1, f_hp=1, f_lp=0.1, gain=1e5, T=1):\n",
    "    Vmagk = abs(vk)\n",
    "    Vmagkm1 = abs(vkm1)\n",
    "    psik = (Vmagk - Vmagkm1 - (f_hp*T/2-1)*psikm1)/(1+f_hp*T/2)\n",
    "    epsilonk = gain*(psik**2)\n",
    "    yk = (T*f_lp*(epsilonk + epsilonkm1) - (T*f_lp - 2)*ykm1)/(2 + T*f_lp)\n",
    "    return yk, psik, epsilonk\n",
    "\n",
    "def adaptive_control(delay, vk, vkmdelay, ukmdelay, thresh, yk):\n",
    "    if (yk > thresh):\n",
    "        uk = delay/2*k * ( vk**2 + vkmdelay**2 ) + ukmdelay\n",
    "    else:\n",
    "        uk = ukmdelay\n",
    "    return uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support function\n",
    "def processing_state(V, G, L):\n",
    "    state = np.array([V, G, L]).T\n",
    "    result = np.zeros((60,3))\n",
    "    result[:state.shape[0],:state.shape[1]] = state #padding with zeros\n",
    "    state = result.copy()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the agent\n",
    "import tensorflow as tf\n",
    "EPS = 1e-6\n",
    "\n",
    "def build_graph(inputs, layer_sizes):\n",
    "    if type(inputs) is tf.Tensor:\n",
    "        inputs = [inputs]\n",
    "    \n",
    "    with tf.variable_scope('layer0'):\n",
    "        state = inputs[0]\n",
    "        gru_cell = tf.contrib.rnn.GRUCell(num_units=32)\n",
    "        outputs, states = tf.nn.dynamic_rnn(gru_cell, state, dtype=tf.float32)\n",
    "        state0 = tf.layers.dense(states.h, 30)\n",
    "        if len(inputs) != 1:\n",
    "            action = inputs[1]\n",
    "            action0 = tf.layers.dense(action, 30)\n",
    "            layer = action0 + state0\n",
    "        else: \n",
    "            layer = state0\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    for i_layer, size in enumerate(layer_sizes, 1):\n",
    "        with tf.variable_scope('layer{0}'.format(i_layer)):\n",
    "            layer = tf.layers.dense(layer, size)\n",
    "            if i_layer < len(layer_sizes) - 1:\n",
    "                layer = tf.nn.relu(layer)\n",
    "    \n",
    "    #layer = tf.squeeze(layer)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "class NNFunction():\n",
    "    def __init__(self, name, input_pls, hidden_layer_sizes):\n",
    "        self._name = name\n",
    "        self._input_pls = input_pls\n",
    "        self._layer_sizes = list(hidden_layer_sizes) + [1]\n",
    "        self._output_t = self.get_output_for(*self._input_pls)\n",
    "    def get_output_for(self, *inputs, reuse=False):\n",
    "        with tf.variable_scope(self._name, reuse=reuse):\n",
    "            value_t = build_graph(inputs=inputs, layer_sizes=self._layer_sizes)\n",
    "        return value_t\n",
    "    \n",
    "    def get_params_internal(self):\n",
    "\n",
    "        scope = tf.get_variable_scope().name\n",
    "        scope += '/' + self._name + '/' if len(scope) else self._name + '/'\n",
    "        return tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope\n",
    "        )\n",
    "    \n",
    "class NNVFunction(NNFunction):\n",
    "    def __init__(self, hidden_layer_sizes=(30, 30), name='vf'):\n",
    "        self._Do = [60,3]\n",
    "        self._obs_pl = tf.placeholder(tf.float32, shape=[None] + self._Do, name='observation')\n",
    "        super(NNVFunction, self).__init__(name, (self._obs_pl,), hidden_layer_sizes)\n",
    "\n",
    "\n",
    "class NNQFunction(NNFunction):\n",
    "    def __init__(self, hidden_layer_sizes=(30, 30), name='qf'):\n",
    "        self._Da = 4\n",
    "        self._Do = [60,3]\n",
    "        self._obs_pl = tf.placeholder(tf.float32, shape=[None]+ self._Do, name='observation')\n",
    "        self._action_pl = tf.placeholder(tf.float32, shape=[None, self._Da], name='action')\n",
    "        super(NNQFunction, self).__init__(name, (self._obs_pl, self._action_pl), hidden_layer_sizes)\n",
    "\n",
    "#############################################################        \n",
    "class Normal():\n",
    "    def __init__(self, hidden_layer_sizes, D, obs, reparameterize, reg):\n",
    "        self._layer_sizes = list(hidden_layer_sizes) + [2*D]\n",
    "        self._reparameterize = reparameterize\n",
    "        self._reg = reg\n",
    "        self._obs = obs\n",
    "        self._D = D\n",
    "        self._create_graph()\n",
    "    def _create_graph(self, LOG_SIG_CAP_MIN=-200, LOG_SIG_CAP_MAX=-0.5):\n",
    "        Dx = self._D\n",
    "        mu_and_logsig_t = build_graph(inputs=self._obs, layer_sizes=self._layer_sizes)\n",
    "        self._mu_t = mu_and_logsig_t[..., :Dx]\n",
    "        self._log_sig_t = tf.clip_by_value(mu_and_logsig_t[..., Dx:], LOG_SIG_CAP_MIN, LOG_SIG_CAP_MAX)\n",
    "\n",
    "        # Tensorflow's multivariate normal distribution supports reparameterization\n",
    "        ds = tf.contrib.distributions\n",
    "        dist = ds.MultivariateNormalDiag(loc=self._mu_t, scale_diag=tf.exp(self._log_sig_t))\n",
    "        x_t = dist.sample()\n",
    "        if not self._reparameterize:\n",
    "            x_t = tf.stop_gradient(x_t)\n",
    "        log_pi_t = dist.log_prob(x_t)\n",
    "\n",
    "        self._dist = dist\n",
    "        self._x_t = x_t\n",
    "        self._log_pi_t = log_pi_t\n",
    "        \n",
    "        reg_loss_t = self._reg * 0.5 * tf.reduce_mean(self._log_sig_t ** 2)\n",
    "        reg_loss_t += self._reg * 0.5 * tf.reduce_mean(self._mu_t ** 2)\n",
    "        self._reg_loss_t = reg_loss_t\n",
    "    \n",
    "    @property\n",
    "    def log_p_t(self):\n",
    "        return self._log_pi_t\n",
    "\n",
    "    @property\n",
    "    def reg_loss_t(self):\n",
    "        return self._reg_loss_t\n",
    "\n",
    "    @property\n",
    "    def x_t(self):\n",
    "        return self._x_t\n",
    "\n",
    "    @property\n",
    "    def mu_t(self):\n",
    "        return self._mu_t\n",
    "\n",
    "    @property\n",
    "    def log_sig_t(self):\n",
    "        return self._log_sig_t\n",
    "    \n",
    "class GaussianPolicy():\n",
    "    def __init__(self, hidden_layer_sizes=(30,30), reparameterize=False, name='policy', squash=True):\n",
    "        self._name = name\n",
    "        self._Do = [60,3]\n",
    "        self._Da = 4\n",
    "        self._obs_pl = tf.placeholder(tf.float32, shape=[None] + self._Do, name='observation')\n",
    "        self._action_pl = tf.placeholder(tf.float32, shape=[None, self._Da], name='action')\n",
    "        self._layer_sizes = hidden_layer_sizes\n",
    "        self._squash = squash\n",
    "        self._reparameterize = reparameterize\n",
    "    def actions_for(self, observations, reuse=tf.AUTO_REUSE, with_log_pis = True):\n",
    "        name = self._name\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            distribution = Normal(hidden_layer_sizes = self._layer_sizes, D = self._Da, obs=(observations,), \n",
    "                                 reparameterize=self._reparameterize, reg=True)\n",
    "        \n",
    "        raw_actions = distribution.x_t\n",
    "        actions = tf.tanh(raw_actions) if self._squash else raw_actions\n",
    "        \n",
    "        if with_log_pis:\n",
    "            log_pis = distribution.log_p_t\n",
    "            if self._squash:\n",
    "                log_pis -= self._squash_correction(raw_actions)\n",
    "            return actions, log_pis\n",
    "        \n",
    "        return actions    \n",
    "    \n",
    "    def get_params_internal(self):\n",
    "\n",
    "        scope = tf.get_variable_scope().name\n",
    "        scope += '/' + self._name + '/' if len(scope) else self._name + '/'\n",
    "        return tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope\n",
    "        )\n",
    "    \n",
    "    def _squash_correction(self, actions):\n",
    "        if not self._squash: return 0\n",
    "        return tf.reduce_sum(tf.log(1 - tf.tanh(actions) ** 2 + EPS), axis=1)\n",
    "    \n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "class agent():\n",
    "    def __init__(self, sess, observations_ph, next_observations_ph, rewards_ph, actions_ph, terminals_ph, name='agent'):\n",
    "        self._observations_ph = observations_ph\n",
    "        self._next_observations_ph = next_observations_ph\n",
    "        self._rewards_ph = rewards_ph\n",
    "        self._actions_ph= actions_ph\n",
    "        self._terminals_ph = terminals_ph\n",
    "        \n",
    "        self._discount = 0.99\n",
    "        self._tau = 0.01\n",
    "        \n",
    "        self._qf_lr = 0.1\n",
    "        self._vf_lr = 0.1\n",
    "        self._policy_lr = 0.01\n",
    "        self._action_prior = 'Normal'\n",
    "        self._reparameterize = False\n",
    "        self._sess = sess\n",
    "        self._training_ops = []\n",
    "        with tf.variable_scope(name):\n",
    "            self._vf = NNVFunction((30,30))\n",
    "            self._qf1 = NNQFunction((30,30), 'qf1')\n",
    "            self._qf2 = NNQFunction((30,30), 'qf2')\n",
    "            self._policy = GaussianPolicy(hidden_layer_sizes=(30,30), reparameterize=self._reparameterize)\n",
    "            self._init_critic_update()\n",
    "            self._init_actor_update()\n",
    "            self._init_target_ops()\n",
    "            \n",
    "        uninit_vars = []\n",
    "        for var in tf.global_variables():\n",
    "            try:\n",
    "                self._sess.run(var)\n",
    "            except tf.errors.FailedPreconditionError:\n",
    "                uninit_vars.append(var)\n",
    "        self._sess.run(tf.variables_initializer(uninit_vars))\n",
    "        \n",
    "        self._init_training()\n",
    "        \n",
    "    def _init_critic_update(self):\n",
    "        self._qf1_t = self._qf1.get_output_for(\n",
    "            self._observations_ph, self._actions_ph, reuse=True)  # N\n",
    "        self._qf2_t = self._qf2.get_output_for(\n",
    "            self._observations_ph, self._actions_ph, reuse=True)  # N\n",
    "\n",
    "        with tf.variable_scope('target'):\n",
    "            vf_next_target_t = self._vf.get_output_for(self._next_observations_ph)  # N\n",
    "            self._vf_target_params = self._vf.get_params_internal()\n",
    "\n",
    "        ys = tf.stop_gradient(\n",
    "            self._rewards_ph +\n",
    "            (1 - self._terminals_ph) * self._discount * vf_next_target_t\n",
    "        )  # N\n",
    "\n",
    "        self._td_loss1_t = 0.5 * tf.reduce_mean((ys - self._qf1_t)**2)\n",
    "        self._td_loss2_t = 0.5 * tf.reduce_mean((ys - self._qf2_t)**2)\n",
    "\n",
    "        qf1_train_op = tf.train.AdamOptimizer(self._qf_lr).minimize(\n",
    "            loss=self._td_loss1_t,\n",
    "            var_list=self._qf1.get_params_internal()\n",
    "        )\n",
    "        qf2_train_op = tf.train.AdamOptimizer(self._qf_lr).minimize(\n",
    "            loss=self._td_loss2_t,\n",
    "            var_list=self._qf2.get_params_internal()\n",
    "        )\n",
    "\n",
    "        self._training_ops.append(qf1_train_op)\n",
    "        self._training_ops.append(qf2_train_op)\n",
    "\n",
    "    def _init_actor_update(self):\n",
    "        actions, log_pi = self._policy.actions_for(observations=self._observations_ph,\n",
    "                                                   with_log_pis=True)\n",
    "\n",
    "        self._vf_t = self._vf.get_output_for(self._observations_ph, reuse=True)  # N\n",
    "        self._vf_params = self._vf.get_params_internal()\n",
    "\n",
    "        policy_prior_log_probs = 0.0\n",
    "        if self._action_prior == 'normal':\n",
    "            D_s = actions.shape.as_list()[-1]\n",
    "            policy_prior = tf.contrib.distributions.MultivariateNormalDiag(\n",
    "                loc=tf.zeros(D_s), scale_diag=tf.ones(D_s))\n",
    "            policy_prior_log_probs = policy_prior.log_prob(actions)\n",
    "        elif self._action_prior == 'uniform':\n",
    "            policy_prior_log_probs = 0.0\n",
    "\n",
    "        log_target1 = self._qf1.get_output_for(\n",
    "            self._observations_ph, actions, reuse=True)  # N\n",
    "        log_target2 = self._qf2.get_output_for(\n",
    "            self._observations_ph, actions, reuse=True)  # N\n",
    "        min_log_target = tf.minimum(log_target1, log_target2)\n",
    "\n",
    "        if self._reparameterize:\n",
    "            policy_kl_loss = tf.reduce_mean(log_pi - log_target1)\n",
    "        else:\n",
    "            policy_kl_loss = tf.reduce_mean(log_pi * tf.stop_gradient(\n",
    "                log_pi - log_target1 + self._vf_t - policy_prior_log_probs))\n",
    "\n",
    "        policy_regularization_losses = tf.get_collection(\n",
    "            tf.GraphKeys.REGULARIZATION_LOSSES,\n",
    "            scope=self._policy.name)\n",
    "        policy_regularization_loss = tf.reduce_sum(\n",
    "            policy_regularization_losses)\n",
    "\n",
    "        policy_loss = (policy_kl_loss\n",
    "                       + policy_regularization_loss)\n",
    "\n",
    "        # We update the vf towards the min of two Q-functions in order to\n",
    "        # reduce overestimation bias from function approximation error.\n",
    "        self._vf_loss_t = 0.5 * tf.reduce_mean((\n",
    "          self._vf_t\n",
    "          - tf.stop_gradient(min_log_target - log_pi + policy_prior_log_probs)\n",
    "        )**2)\n",
    "\n",
    "        policy_train_op = tf.train.AdamOptimizer(self._policy_lr).minimize(\n",
    "            loss=policy_loss,\n",
    "            var_list=self._policy.get_params_internal()\n",
    "        )\n",
    "\n",
    "        vf_train_op = tf.train.AdamOptimizer(self._vf_lr).minimize(\n",
    "            loss=self._vf_loss_t,\n",
    "            var_list=self._vf_params\n",
    "        )\n",
    "\n",
    "        self._training_ops.append(policy_train_op)\n",
    "        self._training_ops.append(vf_train_op)        \n",
    "        self._action = actions\n",
    "        \n",
    "    def _init_target_ops(self):\n",
    "        \"\"\"Create tensorflow operations for updating target value function.\"\"\"\n",
    "\n",
    "        source_params = self._vf_params\n",
    "        target_params = self._vf_target_params\n",
    "\n",
    "        self._target_ops = [\n",
    "            tf.assign(target, (1 - self._tau) * target + self._tau * source)\n",
    "            for target, source in zip(target_params, source_params)\n",
    "        ]\n",
    "\n",
    "    def _init_training(self):\n",
    "        self._sess.run(self._target_ops)\n",
    "\n",
    "    def do_training(self, batch):\n",
    "        \"\"\"Runs the operations for updating training and target ops.\"\"\"\n",
    "\n",
    "        feed_dict = self._get_feed_dict(batch)\n",
    "        self._sess.run(self._training_ops, feed_dict)\n",
    "        self._sess.run(self._target_ops)\n",
    "\n",
    "    def _get_feed_dict(self, batch):\n",
    "        \"\"\"Construct TensorFlow feed_dict from sample batch.\"\"\"\n",
    "\n",
    "        feed_dict = {\n",
    "            self._observations_ph: batch['observations'],\n",
    "            self._actions_ph: batch['actions'],\n",
    "            self._next_observations_ph: batch['next_observations'],\n",
    "            self._rewards_ph: batch['rewards'],\n",
    "            self._terminals_ph: batch['terminals'],\n",
    "        }\n",
    "        return feed_dict\n",
    "   \n",
    "    def action_respond(self, obs):\n",
    "        action = self._action\n",
    "        a = self._sess.run([action], feed_dict={observations_ph: obs})[0][0]\n",
    "        return a\n",
    "    \n",
    "def init_placeholder():\n",
    "    observations_ph = tf.placeholder(tf.float32, shape=(None, 60, 3), name='observation')\n",
    "    next_observations_ph = tf.placeholder(tf.float32, shape=(None, 60, 3), name='next_observation')\n",
    "    rewards_ph = tf.placeholder(tf.float32, shape=(None, 1), name='reward')\n",
    "    actions_ph = tf.placeholder(tf.float32, shape=(None, 4), name='action')\n",
    "    terminals_ph = tf.placeholder(tf.float32, shape=(None, 1), name='terminal')\n",
    "    return observations_ph, next_observations_ph, rewards_ph, actions_ph, terminals_ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_ph, next_observations_ph, rewards_ph, actions_ph, terminals_ph = init_placeholder()\n",
    "sess = tf.Session()\n",
    "agent = agent(sess, observations_ph, next_observations_ph, rewards_ph, actions_ph, terminals_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "batch = {\n",
    "    'observations': np.random.random_sample((5,60,3)),\n",
    "    'actions' : np.random.random_sample((5,4)),\n",
    "    'next_observations': np.random.random_sample((5,60,3)),\n",
    "    'rewards': np.random.random_sample((5,1)),\n",
    "    'terminals': np.array([[1],[1],[1],[1],[1]])\n",
    "    \n",
    "}\n",
    "#agent.do_training(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "\"\"\"import numpy as np \n",
    "policy = GaussianPolicy()\n",
    "observations_ph, next_observations_ph, rewards_ph, actions_ph, terminals_ph = init_placeholder()\n",
    "action = policy.actions_for(observations_ph, with_log_pis = False)\n",
    "#testing\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    a = sess.run([action], feed_dict={observations_ph: np.random.random_sample((5,60,3))})[0]\n",
    "    print(a)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "totalPoints = [[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "############################################\n",
    "########### INIT FOR AGENT #################\n",
    "############################################\n",
    "\n",
    "#init action\n",
    "oldAction = []\n",
    "action = []\n",
    "\n",
    "#init state\n",
    "oldState = []\n",
    "state = []\n",
    "\n",
    "#init reward\n",
    "reward = None\n",
    "\n",
    "mb_state=[]\n",
    "mb_action=[]\n",
    "mb_reward=[]\n",
    "mb_nextstate=[]\n",
    "buffer = ReplayBuffer(300)\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "for ep in range(50):\n",
    "    Load = Origin_Load.copy()\n",
    "    Generation = Origin_Generation.copy()\n",
    "    #this is for permutate the Load and Generation profile on each node\n",
    "    #Load = np.random.permutation(Load.T).T\n",
    "    #Generation = np.random.permutation(Generation.T).T\n",
    "\n",
    "    #this is to permutate the Load and Generation profile\n",
    "    #temp = np.copy(Load[:, 0:Number_of_Inverters])\n",
    "    #Load[:, 0:Number_of_Inverters] = Load[:, Number_of_Inverters:Number_of_Inverters*2]\n",
    "    #Load[:, Number_of_Inverters:Number_of_Inverters*2] = temp\n",
    "    #temp = np.copy(Generation[:, 0:Number_of_Inverters])\n",
    "    #Generation[:, 0:Number_of_Inverters] = Generation[:, Number_of_Inverters:Number_of_Inverters*2]\n",
    "    #Generation[:, Number_of_Inverters:Number_of_Inverters*2] = temp\n",
    "\n",
    "    #Create noise vector\n",
    "    Noise = np.empty([TotalTimeSteps, TotalLoads])\n",
    "    for node in range(TotalLoads):\n",
    "        Noise[:,node] = np.random.randn(TotalTimeSteps) \n",
    "\n",
    "    #Add noise to loads\n",
    "    for node in range(TotalLoads):\n",
    "        Load[:,node] = Load[:,node] + NoiseMultiplyer*Noise[:,node]\n",
    "\n",
    "    if NoiseMultiplyer > 0:\n",
    "        print('Load Interpolation has been done. Noise was added to the load profile.') \n",
    "    else:\n",
    "        print('Load Interpolation has been done. No Noise was added to the load profile.') \n",
    "\n",
    "    MaxGenerationPossible = np.max(Generation, axis = 0)\n",
    "    sbar = MaxGenerationPossible\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #########################################################\n",
    "    ################### RESTART OPENDSS #####################\n",
    "    #########################################################\n",
    "    DSSStart = DSSStartup()\n",
    "    DSSText =DSSStart['dsstext']\n",
    "    DSSSolution = DSSStart['dsssolution']\n",
    "    DSSCircuit = DSSStart['dsscircuit']\n",
    "    DSSObj = DSSStart['dssobj']\n",
    "    DSSMon = DSSCircuit.Monitors\n",
    "    DSSText.command = 'Compile C:\\\\feeders\\\\feeder34_B_NR\\\\feeder34_B_NR.dss'\n",
    "    DSSSolution.Solve()\n",
    "    if not DSSSolution.Converged:\n",
    "        print('Initial Solution Not Converged. Check Model for Convergence')\n",
    "    else:\n",
    "        print('Initial Model Converged. Proceeding to Next Step.')\n",
    "        #Doing this solve command is required for GridPV, that is why the monitors\n",
    "        #go under a reset process\n",
    "        DSSMon.ResetAll\n",
    "        setSolutionParams(DSSObj,'daily',1,1,'off',1000000,30000)\n",
    "        #Easy process to get all names and count of loads, a trick to avoid\n",
    "        #some more lines of code\n",
    "        TotalLoads=DSSCircuit.Loads.Count\n",
    "        AllLoadNames=DSSCircuit.Loads.AllNames\n",
    "        print('OpenDSS Model Compliation Done.')\n",
    "\n",
    "    ############################################\n",
    "    ############ INIT NODES ####################\n",
    "    ############################################\n",
    "    nodes = {}\n",
    "    features = ['Voltage', 'Generation', 'P', 'Q']\n",
    "\n",
    "    for i in range(len(AllLoadNames)):\n",
    "        df = pd.DataFrame(columns=list(range(TotalTimeSteps)),index=features)\n",
    "        nodes[i] = df\n",
    "        nodes[i].loc['Generation'] = Generation[:,i]\n",
    "        nodes[i].loc['P'] = 0\n",
    "        nodes[i].loc['Q'] = 0\n",
    "\n",
    "    ################################################\n",
    "    ############ INIT INVERTERS ####################\n",
    "    ################################################\n",
    "    inverters = {}\n",
    "    features = ['VBP', 'FilterVoltage', 'Generation', 'sbar', 'P_inv', 'Q_inv' ,'counter', 'ime_output', 'ep_output', 'yk', 'upk','uqk']\n",
    "\n",
    "    offset = 5\n",
    "    numberofInverters = Number_of_Inverters\n",
    "\n",
    "    for i in range(len(AllLoadNames)):\n",
    "        inverters[i] = []\n",
    "        if offset-1 < i < numberofInverters + offset:\n",
    "            df = pd.DataFrame(columns=list(range(TotalTimeSteps)),index=features)\n",
    "            df.at['FilterVoltage', 0] = 0\n",
    "            df.loc['Generation'] = Generation[:,i]\n",
    "            df.loc['sbar'] = sbar[i]\n",
    "            df.loc['counter'] = 0\n",
    "            df.loc['ime_output'] = 0\n",
    "            df.loc['ep_output'] = 0\n",
    "            df.loc['yk'] = 0\n",
    "            df.loc['P_inv'] = 0\n",
    "            df.loc['Q_inv'] = 0\n",
    "            df.loc['upk'] = 0\n",
    "            df.loc['uqk'] = 0\n",
    "            inverters[i].append(df)\n",
    "\n",
    "    ############################################\n",
    "    ########### INIT VBPCURVE ##################\n",
    "    ############################################\n",
    "    for i in range(len(AllLoadNames)):\n",
    "        for j in range(len(inverters[i])):\n",
    "            for k in range(TotalTimeSteps):\n",
    "                inverters[i][j].at['VBP',k] = np.array([1.01, 1.03, 1.03, 1.05])\n",
    "\n",
    "    VBPcounter = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    controlInv = list(range(7,8))    \n",
    "    points = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    terminal = False\n",
    "    \n",
    "    for timeStep in range(TotalTimeSteps):\n",
    "        VBPcounter = [x+1 for x in VBPcounter] \n",
    "        ####################################################\n",
    "        ################ run the simulation ################\n",
    "        ####################################################\n",
    "        #for the first steps, we just init voltage value, no pq injection\n",
    "        if timeStep == 0:\n",
    "            for node in range(len(AllLoadNames)):\n",
    "                nodeName = AllLoadNames[node]\n",
    "                setLoadInfo(DSSObj, [nodeName], 'kw', [Load[timeStep, node]])\n",
    "                setLoadInfo(DSSObj, [nodeName], 'kvar', [pf_converted*Load[timeStep, node]])\n",
    "        else:\n",
    "            for node in range(len(AllLoadNames)):\n",
    "                nodeName = AllLoadNames[node]\n",
    "                setLoadInfo(DSSObj, [nodeName], 'kw', [Load[timeStep, node] + nodes[node].at['P', timeStep-1]])\n",
    "                setLoadInfo(DSSObj, [nodeName], 'kvar', [pf_converted*Load[timeStep, node] + nodes[node].at['Q', timeStep-1]])\n",
    "\n",
    "        DSSSolution.Solve()\n",
    "        if (not DSSSolution.Converged):\n",
    "            print('Solution Not Converged at Step:', timeStep)\n",
    "\n",
    "        #get the voltage info\n",
    "        nodeInfo = getLoadInfo(DSSObj, [])\n",
    "        #distribute voltage to node\n",
    "        for i in range(len(nodes)):\n",
    "            node = nodes[i]\n",
    "            node.at['Voltage', timeStep] = nodeInfo[i]['voltagePU']\n",
    "            if timeStep == 0 or timeStep == 1:\n",
    "                node.at['Voltage', timeStep] = 1\n",
    "        #############################################################\n",
    "        #############################################################\n",
    "        #############################################################\n",
    "        if timeStep == TimeStepOfHack:\n",
    "            for node in range(len(AllLoadNames)):\n",
    "                if inverters[node] != []:\n",
    "                    inverter = inverters[node][0]\n",
    "                    #create new inverter\n",
    "                    hackedInv = copy.deepcopy(inverter)\n",
    "                    for k in range(timeStep, TotalTimeSteps):\n",
    "                        hackedInv.at['VBP',k] = np.array([1.01, 1.015, 1.015, 1.02])\n",
    "\n",
    "                    hackedInv.loc['sbar'][timeStep:] = hackedInv.loc['sbar'][timeStep:]*PercentHacked[node]\n",
    "                    hackedInv.loc['Generation'][timeStep:] = hackedInv.loc['Generation'][timeStep:]*PercentHacked[node]\n",
    "                    inverters[node].append(hackedInv)\n",
    "                    #generation and sbar change on the original inverter\n",
    "                    inverter.loc['sbar'][timeStep:] = inverter.loc['sbar'][timeStep:]*(1-PercentHacked[node])\n",
    "                    inverter.loc['Generation'][timeStep:] = inverter.loc['Generation'][timeStep:]*(1-PercentHacked[node])\n",
    "        ########################################################################\n",
    "        ########################################################################\n",
    "        ########################################################################\n",
    "        \n",
    "        if timeStep == TotalTimeSteps-1:\n",
    "            terminal = True\n",
    "            \n",
    "        if timeStep != 0:\n",
    "            for node in range(len(AllLoadNames)):\n",
    "                #if we have inverters at that node then...\n",
    "                if inverters[node] != []:\n",
    "                    invertersNode = inverters[node] #get the list of inverters at that node\n",
    "                    for inverter in invertersNode: #get an inverter at that node\n",
    "                        #increase the counter\n",
    "                        inverter.at['counter',timeStep] = inverter.at['counter',timeStep-1] + 1\n",
    "                        if inverter.at['counter',timeStep-1] == Delay_VoltageSampling[node]:\n",
    "                            inverter.at['counter',timeStep] = 1\n",
    "                            \n",
    "                        #get State\n",
    "                        if (node in controlInv) and (inverter is invertersNode[0]):\n",
    "                            if VBPcounter[node-offset] == Delay_VBPCurveShift[node] or terminal == True:\n",
    "                                if terminal == False:\n",
    "                                    length = Delay_VBPCurveShift[node]\n",
    "                                    V = nodes[node].loc['Voltage',timeStep-length+1:timeStep]\n",
    "                                    G = inverter.loc['Generation', timeStep-length+1:timeStep] \n",
    "                                    L = Load[timeStep-length+1:timeStep+1, node]\n",
    "                                else:\n",
    "                                    length = VBPcounter[node-offset]\n",
    "                                    V = nodes[node].loc['Voltage',timeStep-length+1:]\n",
    "                                    G = inverter.loc['Generation', timeStep-length+1:]\n",
    "                                    L = Load[timeStep-length+1:, node]\n",
    "                                    \n",
    "                                oldState = state\n",
    "                                state = processing_state(V,G,L)\n",
    "                                #reset VBPcounter\n",
    "                                VBPcounter[node-offset] = 0\n",
    "                                #store action                        \n",
    "                                oldAction = copy.deepcopy(action)\n",
    "                                \n",
    "                                #get Action from Agent\n",
    "                                action = agent.action_respond(state.reshape((1,60,3)))\n",
    "                                #apply strict boundary\n",
    "                                if action[1] < action[0]:\n",
    "                                    action[0] = action[1]\n",
    "                                if action[3] < action[2]:\n",
    "                                    action[2] = action[3]\n",
    "                                for k in range(timeStep, TotalTimeSteps):\n",
    "                                    inverter.at['VBP',k] = copy.deepcopy(action*0.1 + 1)\n",
    "                                \n",
    "                                #reward\n",
    "                                #caculate yk for that inverter\n",
    "                                inverter.at['yk', timeStep], inverter.at['ime_output', timeStep], inverter.at['ep_output', timeStep] = voltage_observer(\n",
    "                                         nodes[node].at['Voltage', timeStep], \n",
    "                                         nodes[node].at['Voltage', timeStep-1],\n",
    "                                         inverter.at['ime_output', timeStep-1], \n",
    "                                         inverter.at['ep_output', timeStep-1], \n",
    "                                         inverter.at['yk', timeStep-1])\n",
    "\n",
    "                                numberYks = inverter.loc['yk', timeStep-length+1:timeStep]\n",
    "                                numberYks = numberYks - 0.25\n",
    "                                numberYks = numberYks[numberYks > 0]\n",
    "                                reward = - sum(numberYks**2)\n",
    "                                \n",
    "                                #sum of reward for that ep\n",
    "                                points[node-offset] += reward\n",
    "                                \n",
    "                                \n",
    "                                if oldAction != [] and oldState != []:\n",
    "                                    if terminal: \n",
    "                                        ter = 1\n",
    "                                    else:\n",
    "                                        ter = 0\n",
    "                                    buffer.add(oldState.reshape((60,3)), oldAction, np.array([reward]), np.array([ter]), state.reshape((60,3)))\n",
    "                                \n",
    "                                if buffer.size() > BATCH_SIZE:\n",
    "                                    batch = {}\n",
    "                                    mb_state, mb_action, mb_reward, mb_ter, mb_nextstate = buffer.sample_batch(BATCH_SIZE)\n",
    "                                    batch['observations'] = mb_state\n",
    "                                    batch['actions'] = mb_action\n",
    "                                    batch['rewards'] = mb_reward\n",
    "                                    batch['next_observations'] = mb_nextstate\n",
    "                                    batch['rewards'] = mb_reward\n",
    "                                    batch['terminals'] = mb_ter\n",
    "                                    start_time_train = time.time()\n",
    "                                    agent.do_training(batch)\n",
    "                                    print(\"--- %s seconds ---\" % (time.time() - start_time_train))\n",
    "                        #################################################                        \n",
    "                        #################################################\n",
    "                        #execute action\n",
    "                        inverter.at['Q_inv', timeStep], inverter.at['P_inv', timeStep], inverter.at['FilterVoltage', timeStep] = inverter_qp_injection(\n",
    "                            inverter.at['counter',timeStep],\n",
    "                            nodes[node].at['Voltage', timeStep],\n",
    "                            nodes[node].at['Voltage', timeStep-1],\n",
    "                            inverter.at['FilterVoltage', timeStep-1],\n",
    "                            inverter.at['Generation', timeStep],\n",
    "                            inverter.at['VBP', timeStep],\n",
    "                            inverter.at['sbar', timeStep],\n",
    "                            Delay_VoltageSampling[node])\n",
    "                        \n",
    "                        inverter.at['yk', timeStep], inverter.at['ime_output', timeStep], inverter.at['ep_output', timeStep] = voltage_observer(\n",
    "                                         nodes[node].at['Voltage', timeStep], \n",
    "                                         nodes[node].at['Voltage', timeStep-1],\n",
    "                                         inverter.at['ime_output', timeStep-1], \n",
    "                                         inverter.at['ep_output', timeStep-1], \n",
    "                                         inverter.at['yk', timeStep-1])\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        #caculate P Q injection at the node\n",
    "                        nodes[node].at['P', timeStep] += inverter.at['P_inv', timeStep]\n",
    "                        nodes[node].at['Q', timeStep] += inverter.at['Q_inv', timeStep]\n",
    "\n",
    "    ######### drawing #####################\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        totalPoints[i].append(points[i])\n",
    "    print(max(totalPoints[7-offset]))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    f = plt.figure()\n",
    "    f.set_figheight(4)\n",
    "    f.set_figwidth(20)\n",
    "    f.add_subplot(1,4,1)\n",
    "    plt.plot(nodes[7].loc['Voltage'], marker='o')\n",
    "    \n",
    "    for i in range(5,18):\n",
    "        x = inverters[i][0].loc['VBP']\n",
    "        y=np.zeros([len(x),x[0].shape[0]])\n",
    "        for i in range(len(x)):\n",
    "            y[i,:] = x[i]\n",
    "        f.add_subplot(1,4,2)\n",
    "        plt.plot(y[:,0], 'r')\n",
    "        plt.plot(y[:,1], 'y')\n",
    "        plt.plot(y[:,2], 'b')\n",
    "        plt.plot(y[:,3], 'k')\n",
    "    \n",
    "    f.add_subplot(1,4,3)\n",
    "    for i in range(7,8):\n",
    "        yk = inverters[i][0].loc['yk']\n",
    "        plt.plot(yk)\n",
    "    plt.axhline(y=0.25, color='r', linestyle='-')\n",
    "    f.add_subplot(1,4,4)\n",
    "    plt.plot(totalPoints[7-offset])\n",
    "    plt.show(block=True)\n",
    "    f.savefig(str(ep) + '.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
