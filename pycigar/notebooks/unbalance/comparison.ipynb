{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycigar\n",
    "from pycigar.utils.registry import make_create_env\n",
    "from pycigar.utils.input_parser import input_parser\n",
    "from pycigar.utils.logging import logger\n",
    "from pycigar.utils.output import plot_new\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from ray.rllib.models.tf.tf_action_dist import DiagGaussian\n",
    "from ray.rllib.models.catalog import ModelCatalog\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_defaults(sim_params):\n",
    "    for node in sim_params['scenario_config']['nodes']:\n",
    "        for d in node['devices']:\n",
    "            d['adversary_controller'] = 'unbalanced_fixed_controller'\n",
    "            name = d['name']\n",
    "            c = np.array(d['custom_configs']['default_control_setting'])\n",
    "            if name.endswith('a'):\n",
    "                c = c - 0.02\n",
    "            elif name.endswith('b'):\n",
    "                c = c + 0.02\n",
    "            elif name.endswith('c'):\n",
    "                c = c - 0.01\n",
    "            d['custom_configs']['default_control_setting'] = c\n",
    "            \n",
    "def set_hack_percent(sim_params, hack=0.45):\n",
    "    for node in sim_params['scenario_config']['nodes']:\n",
    "        for d in node['devices']:\n",
    "            d['hack'] = [250, hack, 500]\n",
    "    \n",
    "def get_config(env_name='CentralControlPhaseSpecificContinuousPVInverterEnv'):\n",
    "    pycigar_params = {'exp_tag': 'cooperative_multiagent_ppo',\n",
    "                      'env_name': env_name,\n",
    "                      'simulator': 'opendss'}\n",
    "    create_env, env_name = make_create_env(pycigar_params, version=0)\n",
    "    register_env(env_name, create_env)\n",
    "\n",
    "    misc_inputs_path = pycigar.DATA_DIR + \"/ieee37busdata_regulator_attack/misc_inputs.csv\"\n",
    "    dss_path = pycigar.DATA_DIR + \"/ieee37busdata_regulator_attack/ieee37.dss\"\n",
    "    load_solar_path = pycigar.DATA_DIR + \"/ieee37busdata_regulator_attack/load_solar_data.csv\"\n",
    "    breakpoints_path = pycigar.DATA_DIR + \"/ieee37busdata_regulator_attack/breakpoints.csv\"\n",
    "    sim_params = input_parser(misc_inputs_path, dss_path, load_solar_path, breakpoints_path)\n",
    "    \n",
    "    eval_start = 100\n",
    "    sim_params['scenario_config']['start_end_time'] = [eval_start, eval_start + 750]\n",
    "    sim_params['scenario_config']['multi_config'] = False\n",
    "    del sim_params['attack_randomization']\n",
    "    adjust_defaults(sim_params)\n",
    "    \n",
    "    return create_env, sim_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util: eval policy checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_baseline(eval_start=100, hack=0.45):\n",
    "    create_env, sim_params = get_config('CentralControlPhaseSpecificContinuousPVInverterEnv')\n",
    "    set_hack_percent(sim_params, hack)\n",
    "    result_dict = {'stds': []}\n",
    "    \n",
    "    sim_params['scenario_config']['start_end_time'] = [eval_start, eval_start + 750]\n",
    "    test_env = create_env(sim_params)\n",
    "\n",
    "    done = False\n",
    "    obs = test_env.reset()\n",
    "    obs = obs.tolist()\n",
    "    while not done:\n",
    "        obs, r, done, _ = test_env.step(np.array([0, 0, 0]))\n",
    "        obs = obs.tolist()\n",
    "\n",
    "    Logger = logger()\n",
    "    f = plot_new(Logger.log_dict, Logger.custom_metrics, None, unbalance=True)\n",
    "    result_dict['figure'] = f\n",
    "    plt.close(f)\n",
    "    inv = [k for k in Logger.log_dict if k.startswith('inverter_s701') or k.startswith('inverter_s728')][0]\n",
    "\n",
    "    result_dict['u_sum'] = sum(Logger.log_dict[inv]['u'])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dir_discrete(best_dir, eval_start=100, hack=0.45):\n",
    "    create_env, sim_params = get_config('CentralControlPhaseSpecificPVInverterEnv')\n",
    "    set_hack_percent(sim_params, hack)\n",
    "    result_dict = {'stds': [], 'dir': best_dir.parent}\n",
    "    params = json.loads((best_dir.parent / 'params.json').read_text())['config']['evaluation_config']['env_config']\n",
    "    params['scenario_config'] = sim_params['scenario_config']\n",
    "    sim_params['M'] = params['M']\n",
    "    sim_params['N'] = params['N']\n",
    "    sim_params['P'] = params['P']\n",
    "    \n",
    "    sim_params['scenario_config']['start_end_time'] = [eval_start, eval_start + 750]\n",
    "\n",
    "    infos = json.loads((best_dir / 'info.json').read_text())\n",
    "    result_dict['epoch'] = infos['epoch']\n",
    "\n",
    "    policy = tf.saved_model.load(str(best_dir / f'policy'))\n",
    "    infer = policy.signatures['serving_default']\n",
    "\n",
    "    test_env = create_env(sim_params)\n",
    "    action_dist, _ = ModelCatalog.get_action_dist(\n",
    "        test_env.action_space, config={}, dist_type=None, framework='tf')\n",
    "    #assert isinstance(action_dist, DiagGaussian.__class__), 'For now only continuous gaussian action are supported'\n",
    "\n",
    "    done = False\n",
    "    obs = test_env.reset()\n",
    "    obs = obs.tolist()\n",
    "    while not done:\n",
    "        out = infer(\n",
    "            prev_reward=tf.constant([0.], tf.float32),\n",
    "            observations=tf.constant([obs], tf.float32),\n",
    "            is_training=tf.constant(False),\n",
    "            seq_lens=tf.constant([0], tf.int32),\n",
    "            prev_action=tf.constant([0], tf.int64)\n",
    "        )['behaviour_logits'].numpy()\n",
    "        dist = action_dist(inputs=out, model=None)\n",
    "        act = dist.deterministic_sample().numpy().batches\n",
    "\n",
    "        obs, r, done, _ = test_env.step(act)\n",
    "        obs = obs.tolist()\n",
    "\n",
    "    Logger = logger()\n",
    "    f = plot_new(Logger.log_dict, Logger.custom_metrics, infos['epoch'], unbalance=True)\n",
    "    result_dict['figure'] = f\n",
    "    plt.close(f)\n",
    "\n",
    "    inv = [k for k in Logger.log_dict if k.startswith('inverter_s701') or k.startswith('inverter_s728')][0]\n",
    "\n",
    "    result_dict['u_sum'] = sum(Logger.log_dict[inv]['u'])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dir_continuous(best_dir, eval_start=100, hack=0.45):\n",
    "    create_env, sim_params = get_config('CentralControlPhaseSpecificContinuousPVInverterEnv')\n",
    "    set_hack_percent(sim_params, hack)\n",
    "    result_dict = {'stds': [], 'dir': best_dir.parent}\n",
    "    params = json.loads((best_dir.parent / 'params.json').read_text())['config']['evaluation_config']['env_config']\n",
    "    params['scenario_config'] = sim_params['scenario_config']\n",
    "    sim_params['M'] = params['M']\n",
    "    sim_params['N'] = params['N']\n",
    "    sim_params['P'] = params['P']\n",
    "    \n",
    "    sim_params['scenario_config']['start_end_time'] = [eval_start, eval_start + 750]\n",
    "\n",
    "    infos = json.loads((best_dir / 'info.json').read_text())\n",
    "    result_dict['epoch'] = infos['epoch']\n",
    "\n",
    "    policy = tf.saved_model.load(str(best_dir / f'policy_{infos[\"epoch\"]}'))\n",
    "    infer = policy.signatures['serving_default']\n",
    "\n",
    "    test_env = create_env(sim_params)\n",
    "    action_dist, _ = ModelCatalog.get_action_dist(\n",
    "        test_env.action_space, config={}, dist_type=None, framework='tf')\n",
    "    assert isinstance(action_dist, DiagGaussian.__class__), 'For now only continuous gaussian action are supported'\n",
    "\n",
    "    done = False\n",
    "    obs = test_env.reset()\n",
    "    obs = obs.tolist()\n",
    "    while not done:\n",
    "        out = infer(\n",
    "            prev_reward=tf.constant([0.], tf.float32),\n",
    "            observations=tf.constant([obs], tf.float32),\n",
    "            is_training=tf.constant(False),\n",
    "            seq_lens=tf.constant([0], tf.int32),\n",
    "            prev_action=tf.constant([0], tf.float32)\n",
    "        )['behaviour_logits'].numpy()\n",
    "        dist = action_dist(inputs=out, model=None)\n",
    "        act, std = dist.deterministic_sample().numpy().flatten(), dist.std.numpy().flatten()\n",
    "        result_dict['stds'].append(std)\n",
    "\n",
    "        obs, r, done, _ = test_env.step(act)\n",
    "        obs = obs.tolist()\n",
    "\n",
    "    Logger = logger()\n",
    "    f = plot_new(Logger.log_dict, Logger.custom_metrics, infos['epoch'], unbalance=True)\n",
    "    result_dict['figure'] = f\n",
    "    inv = [k for k in Logger.log_dict if k.startswith('inverter_s701') or k.startswith('inverter_s728')][0]\n",
    "\n",
    "    result_dict['u_sum'] = sum(Logger.log_dict[inv]['u'])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dir_multi(best_dir, eval_start=100, hack=0.45):\n",
    "    create_env, sim_params = get_config('PhaseSpecificContinuousMultiEnv')\n",
    "    set_hack_percent(sim_params, hack)\n",
    "    result_dict = {'dir': best_dir.parent}\n",
    "    params = json.loads((best_dir.parent / 'params.json').read_text())['config']['evaluation_config']['env_config']\n",
    "    params['scenario_config'] = sim_params['scenario_config']\n",
    "    sim_params['M'] = params['M']\n",
    "    sim_params['N'] = params['N']\n",
    "    sim_params['P'] = params['P']\n",
    "    \n",
    "    sim_params['scenario_config']['start_end_time'] = [eval_start, eval_start + 750]\n",
    "\n",
    "    infos = json.loads((best_dir / 'info.json').read_text())\n",
    "    result_dict['epoch'] = infos['epoch']\n",
    "\n",
    "    policy_a = tf.saved_model.load(str(best_dir / 'policy/phase_a'))\n",
    "    infer_a = policy_a.signatures['serving_default']\n",
    "    policy_b = tf.saved_model.load(str(best_dir / 'policy/phase_b'))\n",
    "    infer_b = policy_b.signatures['serving_default']\n",
    "    policy_c = tf.saved_model.load(str(best_dir / 'policy/phase_c'))\n",
    "    infer_c = policy_c.signatures['serving_default']\n",
    "\n",
    "    test_env = create_env(sim_params)\n",
    "    action_dist, _ = ModelCatalog.get_action_dist(\n",
    "        test_env.action_space, config={}, dist_type=None, framework='tf')\n",
    "    assert isinstance(action_dist, DiagGaussian.__class__), 'For now only continuous gaussian action are supported'\n",
    "    done = {'__all__': False}\n",
    "    obs = test_env.reset()\n",
    "    while not done['__all__']:        \n",
    "        act = {}\n",
    "        for k in obs:\n",
    "            if k.endswith('a'):\n",
    "                infer = infer_a\n",
    "            elif k.endswith('b'):\n",
    "                infer = infer_b\n",
    "            elif k.endswith('c'):\n",
    "                infer = infer_c\n",
    "            else:\n",
    "                act[k] = np.array([0])\n",
    "                continue\n",
    "            out = infer(\n",
    "                prev_reward=tf.constant([0.], tf.float32),\n",
    "                observations=tf.constant([obs[k]], tf.float32),\n",
    "                is_training=tf.constant(False),\n",
    "                seq_lens=tf.constant([0], tf.int32),\n",
    "                prev_action=tf.constant([0], tf.float32)\n",
    "            )['action_dist_inputs'].numpy()\n",
    "            dist = action_dist(inputs=out, model=None)\n",
    "            a, std = dist.deterministic_sample().numpy().flatten(), dist.std.numpy().flatten()\n",
    "            act[k] = a\n",
    "                \n",
    "        obs, r, done, _ = test_env.step(act)\n",
    "\n",
    "    Logger = logger()\n",
    "    f = plot_new(Logger.log_dict, Logger.custom_metrics, infos['epoch'], unbalance=True)\n",
    "    result_dict['figure'] = f\n",
    "    plt.close(f)\n",
    "    inv = [k for k in Logger.log_dict if k.startswith('inverter_s701') or k.startswith('inverter_s728')][0]\n",
    "\n",
    "    result_dict['u_sum'] = sum(Logger.log_dict[inv]['u'])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous gridearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib agg\n",
    "# %matplotlib agg\n",
    "\n",
    "# SAVE_PATH = \n",
    "# results = []\n",
    "# for best_dir in Path(SAVE_PATH).expanduser().rglob('best/'):\n",
    "#     print(f'Evaluating {best_dir.parent}')\n",
    "#     result_dict = eval_dir(best_dir);\n",
    "#     results.append(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([d['u_sum'] for d in results])\n",
    "# print(np.argmin([d['u_sum'] for d in results]))\n",
    "# results[np.argmin([d['u_sum'] for d in results])]['dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from multiprocessing import Pool, Queue\n",
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "start_times = list(range(0, 14000, 1800))\n",
    "hacks = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "times_hack_product = list(itertools.product(start_times, hacks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "baseline_u = pd.DataFrame(index=start_times, columns=hacks)\n",
    "\n",
    "with Pool(2) as p:\n",
    "    res = p.starmap(eval_baseline, times_hack_product)\n",
    "    \n",
    "for r, (eval_start, hack) in zip(res, times_hack_product):\n",
    "    baseline_u.at[eval_start, hack] = r['u_sum']\n",
    "    \n",
    "del res\n",
    "\n",
    "with open('baseline.pickle', 'wb') as f:\n",
    "    pickle.dump(baseline_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "best_dir = Path('/home/alexandre/dev/LBL/lrc/results_continuous/main/run_train/run_train_0_M=1000,M=1000,N=50,P=100_2020-06-23_08-35-33sqg4e1f9/best')\n",
    "cont_u = pd.DataFrame(index=start_times, columns=hacks)\n",
    "\n",
    "with Pool(2) as p:\n",
    "    res = p.starmap(partial(eval_dir_continuous, best_dir), times_hack_product)\n",
    "    \n",
    "for r, (eval_start, hack) in zip(res, times_hack_product):\n",
    "    cont_u.at[eval_start, hack] = r\n",
    "\n",
    "del res\n",
    "    \n",
    "with open('cont.pickle', 'wb') as f:\n",
    "    pickle.dump(cont_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "best_dir = Path('/home/alexandre/dev/LBL/lrc/results_discrete/best')\n",
    "discrete_u = pd.DataFrame(index=start_times, columns=hacks)\n",
    "\n",
    "with Pool(2) as p:\n",
    "    res = p.starmap(partial(eval_dir_discrete, best_dir), times_hack_product)\n",
    "    \n",
    "for r, (eval_start, hack) in zip(res, times_hack_product):\n",
    "    discrete_u.at[eval_start, hack] = r\n",
    "\n",
    "del res\n",
    "\n",
    "with open('discrete.pickle', 'wb') as f:\n",
    "    pickle.dump(discrete_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "multi_u = pd.DataFrame(index=start_times, columns=hacks)\n",
    "best_dir = Path('~/dev/LBL/lrc/results_multiagent_2/best').expanduser()\n",
    "\n",
    "with Pool(2) as p:\n",
    "    res = p.starmap(partial(eval_dir_multi, best_dir), times_hack_product)\n",
    "    \n",
    "for r, (eval_start, hack) in zip(res, times_hack_product):\n",
    "    multi_u.at[eval_start, hack] = r\n",
    "\n",
    "del res\n",
    "   \n",
    "with open('multi.pickle', 'wb') as f:\n",
    "    pickle.dump(multi_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "multi_new_u = pd.DataFrame(index=start_times, columns=hacks)\n",
    "best_dir = Path('~/dev/LBL/lrc/results_multiagent_new_r/best').expanduser()\n",
    "        \n",
    "with Pool(2) as p:\n",
    "    res = p.starmap(partial(eval_dir_multi, best_dir), times_hack_product)\n",
    "    \n",
    "for r, (eval_start, hack) in zip(res, times_hack_product):\n",
    "    multi_new_u.at[eval_start, hack] = r\n",
    "\n",
    "del res\n",
    "        \n",
    "with open('multi_new_r.pickle', 'wb') as f:\n",
    "    pickle.dump(multi_new_u, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "with open(\"baseline.pickle\", \"rb\") as f:\n",
    "    baseline_u = pickle.load(f)\n",
    "with open(\"multi.pickle\", \"rb\") as f:\n",
    "    multi_u = pickle.load(f) - baseline_u\n",
    "with open(\"multi_new_r.pickle\", \"rb\") as f:\n",
    "    multi_new_r_u = pickle.load(f) - baseline_u\n",
    "with open(\"cont.pickle\", \"rb\") as f:\n",
    "    cont_u = pickle.load(f) - baseline_u\n",
    "with open(\"discrete.pickle\", \"rb\") as f:\n",
    "    discrete_u = pickle.load(f) - baseline_u\n",
    "\n",
    "\n",
    "times_index_str = ['9:00', '9:30', '10:00', '10:30', '11:00', '11:30', '12:00', '12:30']\n",
    "# discrete_u = [6.0444, 5.1379, 4.7188, 5.6285, 6.3616, 6.2202, 6.6645, 6.0929]\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(ncols=3, nrows=3, figsize=(30, 30))\n",
    "axs = np.array(axs).flatten()\n",
    "for i, h in enumerate(hacks):\n",
    "    axs[i].plot(times_index_str, discrete_u.loc[:, h].to_numpy(), marker='s', lw=2, ls='-.', markersize=15, label='central discrete')\n",
    "    axs[i].plot(times_index_str, cont_u.loc[:, h].to_numpy(), marker='s', lw=2, ls='-.', markersize=15, label='central continuous')\n",
    "    axs[i].plot(times_index_str, multi_u.loc[:, h].to_numpy(), marker='D', lw=2, ls='-.', markersize=15, label='multiagent continuous')\n",
    "    axs[i].plot(times_index_str, multi_new_r_u.loc[:, h].to_numpy(), marker='D', lw=2, ls='-.', markersize=15, label='multiagent continuous (new r)')\n",
    "\n",
    "    axs[i].plot(times_index_str, (baseline_u - baseline_u).loc[:, h].to_numpy(), marker='', c='r', lw=6, alpha=0.5, markersize=15, label='baseline (no action)')\n",
    "\n",
    "    axs[i].set_title(f'hack {h}', fontsize=25)\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    axs[i].set_xlabel('day time', fontsize=15)\n",
    "    axs[i].set_ylabel('unbalance diff to baseline', fontsize=15);\n",
    "    #axs[i].set_ylim([-8.5,1])\n",
    "\n",
    "axs[0].legend(prop={'size': 25}, loc=(0, 1.05), frameon=False, ncol=2, columnspacing=2.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycigar2",
   "language": "python",
   "name": "pycigar2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
