{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " import numpy as np\n",
    "import time\n",
    "import gym\n",
    "import queue\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.evaluation.worker_set import WorkerSet\n",
    "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
    "from ray.rllib.execution.concurrency_ops import Concurrently, Enqueue, Dequeue\n",
    "from ray.rllib.execution.metric_ops import StandardMetricsReporting\n",
    "from ray.rllib.execution.replay_ops import StoreToReplayBuffer, Replay\n",
    "from ray.rllib.execution.rollout_ops import ParallelRollouts, AsyncGradients, \\\n",
    "    ConcatBatches\n",
    "from ray.rllib.execution.train_ops import TrainOneStep, ComputeGradients, \\\n",
    "    AverageGradients\n",
    "from ray.rllib.execution.replay_buffer import LocalReplayBuffer, \\\n",
    "    ReplayActor\n",
    "from ray.rllib.policy.sample_batch import SampleBatch, DEFAULT_POLICY_ID, \\\n",
    "    MultiAgentBatch\n",
    "from ray.util.iter import LocalIterator, from_range\n",
    "from ray.util.iter_metrics import SharedMetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palice_replay_buffer import PADICELocalReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_workers(n):\n",
    "    local = RolloutWorker(\n",
    "        env_creator=lambda _: gym.make(\"CartPole-v1\"),\n",
    "        policy=PPOTFPolicy,\n",
    "        rollout_fragment_length=100)\n",
    "    remotes = [\n",
    "        RolloutWorker.as_remote().remote(\n",
    "            env_creator=lambda _: gym.make(\"CartPole-v1\"),\n",
    "            policy=PPOTFPolicy,\n",
    "            rollout_fragment_length=100) for _ in range(n)\n",
    "    ]\n",
    "    workers = WorkerSet._from_existing(local, remotes)\n",
    "    return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-07-12 11:59:25,846\tINFO resource_spec.py:212 -- Starting Ray with 5.27 GiB memory available for workers and up to 2.66 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n2020-07-12 11:59:26,078\tWARNING services.py:923 -- Redis failed to start, retrying now.\n2020-07-12 11:59:26,335\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n2020-07-12 11:59:28,256\tINFO rollout_worker.py:941 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fec3845f810>}\n2020-07-12 11:59:28,257\tINFO rollout_worker.py:942 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fec3845f510>}\n2020-07-12 11:59:28,258\tINFO rollout_worker.py:413 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fec38457c90>}\n2020-07-12 11:59:28,260\tINFO rollout_worker.py:526 -- Generating sample batch of size 100\n2020-07-12 11:59:28,261\tINFO sampler.py:466 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=0.003, max=0.036, mean=0.016)}}\n2020-07-12 11:59:28,261\tINFO sampler.py:467 -- Info return from env: {0: {'agent0': None}}\n2020-07-12 11:59:28,263\tINFO sampler.py:643 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=0.003, max=0.036, mean=0.016)\n2020-07-12 11:59:28,264\tINFO sampler.py:647 -- Filtered obs: np.ndarray((4,), dtype=float64, min=0.003, max=0.036, mean=0.016)\n2020-07-12 11:59:28,266\tINFO sampler.py:792 -- Inputs to compute_actions():\n\n{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n                                  'env_id': 0,\n                                  'info': None,\n                                  'obs': np.ndarray((4,), dtype=float64, min=0.003, max=0.036, mean=0.016),\n                                  'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n                                  'prev_reward': 0.0,\n                                  'rnn_state': []},\n                        'type': 'PolicyEvalData'}]}\n\n2020-07-12 11:59:28,267\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n2020-07-12 11:59:28,303\tINFO sampler.py:834 -- Outputs of compute_actions():\n\n{ 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n                      [],\n                      { 'action_dist_inputs': np.ndarray((1, 2), dtype=float32, min=-0.0, max=0.0, mean=-0.0),\n                        'action_logp': np.ndarray((1,), dtype=float32, min=-0.693, max=-0.693, mean=-0.693),\n                        'action_prob': np.ndarray((1,), dtype=float32, min=0.5, max=0.5, mean=0.5),\n                        'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}\n\n2020-07-12 11:59:28,327\tINFO sample_batch_builder.py:186 -- Trajectory fragment after postprocess_trajectory():\n\n{ 'agent0': { 'data': { 'action_dist_inputs': np.ndarray((18, 2), dtype=float32, min=-0.005, max=0.003, mean=-0.001),\n                        'action_logp': np.ndarray((18,), dtype=float32, min=-0.696, max=-0.689, mean=-0.693),\n                        'action_prob': np.ndarray((18,), dtype=float32, min=0.498, max=0.502, mean=0.5),\n                        'actions': np.ndarray((18,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n                        'advantages': np.ndarray((18,), dtype=float32, min=0.995, max=16.549, mean=8.979),\n                        'agent_index': np.ndarray((18,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n                        'dones': np.ndarray((18,), dtype=bool, min=0.0, max=1.0, mean=0.056),\n                        'eps_id': np.ndarray((18,), dtype=int64, min=771047878.0, max=771047878.0, mean=771047878.0),\n                        'infos': np.ndarray((18,), dtype=object, head={}),\n                        'new_obs': np.ndarray((18, 4), dtype=float32, min=-1.18, max=0.818, mean=-0.067),\n                        'obs': np.ndarray((18, 4), dtype=float32, min=-1.18, max=0.818, mean=-0.06),\n                        'prev_actions': np.ndarray((18,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n                        'prev_rewards': np.ndarray((18,), dtype=float32, min=0.0, max=1.0, mean=0.944),\n                        'rewards': np.ndarray((18,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n                        't': np.ndarray((18,), dtype=int64, min=0.0, max=17.0, mean=8.5),\n                        'unroll_id': np.ndarray((18,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n                        'value_targets': np.ndarray((18,), dtype=float32, min=1.0, max=16.549, mean=8.983),\n                        'vf_preds': np.ndarray((18,), dtype=float32, min=-0.0, max=0.006, mean=0.004)},\n              'type': 'SampleBatch'}}\n\n2020-07-12 11:59:28,404\tINFO rollout_worker.py:559 -- Completed sample batch:\n\n{ 'data': { 'action_dist_inputs': np.ndarray((100, 2), dtype=float32, min=-0.005, max=0.005, mean=-0.0),\n            'action_logp': np.ndarray((100,), dtype=float32, min=-0.698, max=-0.689, mean=-0.694),\n            'action_prob': np.ndarray((100,), dtype=float32, min=0.498, max=0.502, mean=0.5),\n            'actions': np.ndarray((100,), dtype=int64, min=0.0, max=1.0, mean=0.43),\n            'advantages': np.ndarray((100,), dtype=float32, min=0.993, max=17.383, mean=7.263),\n            'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n            'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.07),\n            'eps_id': np.ndarray((100,), dtype=int64, min=136858766.0, max=1682180563.0, mean=675552810.89),\n            'infos': np.ndarray((100,), dtype=object, head={}),\n            'new_obs': np.ndarray((100, 4), dtype=float32, min=-2.092, max=2.594, mean=0.009),\n            'obs': np.ndarray((100, 4), dtype=float32, min=-1.951, max=2.485, mean=0.006),\n            'prev_actions': np.ndarray((100,), dtype=int64, min=0.0, max=1.0, mean=0.39),\n            'prev_rewards': np.ndarray((100,), dtype=float32, min=0.0, max=1.0, mean=0.92),\n            'rewards': np.ndarray((100,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n            't': np.ndarray((100,), dtype=int64, min=0.0, max=18.0, mean=6.61),\n            'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n            'value_targets': np.ndarray((100,), dtype=float32, min=0.995, max=17.383, mean=7.263),\n            'vf_preds': np.ndarray((100,), dtype=float32, min=-0.007, max=0.007, mean=0.0)},\n  'type': 'SampleBatch'}\n\n"
    }
   ],
   "source": [
    "ray.init()\n",
    "buf = PADICELocalReplayBuffer(\n",
    "    num_shards=1,\n",
    "    learning_starts=200,\n",
    "    buffer_size=1000,\n",
    "    replay_batch_size=100,\n",
    "    prioritized_replay_alpha=0.6,\n",
    "    prioritized_replay_beta=0.4,\n",
    "    prioritized_replay_eps=0.0001)\n",
    "assert buf.replay() is None\n",
    "\n",
    "workers = make_workers(0)\n",
    "a = ParallelRollouts(workers, mode=\"bulk_sync\")\n",
    "b = a.for_each(StoreToReplayBuffer(local_buffer=buf))\n",
    "\n",
    "next(b)\n",
    "assert buf.replay() is None  # learning hasn't started yet\n",
    "next(b)\n",
    "assert buf.replay().count == 100\n",
    "\n",
    "replay_op = Replay(local_buffer=buf)\n",
    "assert next(replay_op).count == 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "replay init buffer:  {'added_count': 13, 'sampled_count': 200, 'est_size_bytes': 2717, 'num_entries': 13}\nreplay buffer:  {'added_count': 200, 'sampled_count': 200, 'est_size_bytes': 55400, 'num_entries': 200}\n"
    }
   ],
   "source": [
    "print(\"replay init buffer: \", buf.replay_init_buffers[DEFAULT_POLICY_ID].stats())\n",
    "print(\"replay buffer: \", buf.replay_buffers[DEFAULT_POLICY_ID].stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitpycigartestconda6f494e4b862a4280a0a0055c3626089c",
   "display_name": "Python 3.7.6 64-bit ('pycigar-test': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}